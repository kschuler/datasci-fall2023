{"title":"Model Accuracy","markdown":{"yaml":{"title":"Model Accuracy","subtitle":"Data Science for Studying Language and the Mind","author":"Katie Schuler","date":"10-24-2023","echo":true,"format":{"revealjs":{"theme":"dark","slide-number":true,"incremental":true,"footer":"[https://kathrynschuler.com/datasci](https://kathrynschuler.com/datasci/)"}}},"headingText":"setup data","containsRefs":false,"markdown":"\n\n```{r}\n#| echo: false\n#| message: false\nlibrary(tidyverse)\nlibrary(modelr)\nlibrary(infer)\nlibrary(knitr)\ntheme_set(theme_classic(base_size = 20))\nset.seed(60)\n\ndata <- tibble(\n    x = c(1, 2, 3, 4, 5),\n    y = c(1.2, 2.5, 2.3, 3.1, 4.4)\n)\n\ndata2 <- tibble(\n    x = c(1, 2, 3, 4, 5),\n    y = c(2, 2.5, 3.3, 4.1, 6.4)\n\n\n)\n\n```\n\n\n## You are not alone\n\n![by Allison Horst](/include/figures/Rlearning.png)\n\n## You are `here` {.smaller} \n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n##### Data science with R \n::: {.nonincremental}\n- Hello, world!\n- R basics\n- Data importing\n- Data visualization\n- Data wrangling \n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### Stats & Model buidling\n::: {.nonincremental}\n- Sampling distribution\n- Hypothesis testing\n- Model specification\n- Model fitting\n- `Model accuracy`\n- Model reliability\n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### More advanced \n::: {.nonincremental}\n\n- Classification\n- Feature engineering (preprocessing) \n- Inference for regression\n- Mixed-effect models\n::: \n:::\n\n::::\n\n## Model building overview {.smaller}\n\n- **Model specification**: what is the form?\n- **Model fitting**: you have the form, how do you guess the free parameters? \n- **Model accuracy**: you've estimated the parameters, how well does that model describe your data? \n- **Model reliability**: when you estimate the parameters, there is some uncertainty on them\n\n## Dataset {.smaller}\n\n```{r}\nlibrary(languageR)\nglimpse(english)\n```\n\n## Quantifying model accuracy {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n```{r}\n#| echo: false\nyoung_nouns <- filter(english, WordCategory == 'N', AgeSubject == \"young\")\n\nyoung_nouns_sample <- young_nouns %>% slice_sample(n = 20)\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n    labs(caption = \"sample of 20 participants\")\n```\n\n::: \n::: {.column width=\"40%\"}\n\n- We can visualize to get a sense of accuracy\n- But want to `quantify` accuracy (determine whether model is useful or how it compares to other models)\n\n:::\n::::\n\n\n\n## Quantifying model accuracy {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n```{r}\n#| echo: false\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE)\n```\n\n::: \n::: {.column width=\"40%\"}\n\n- **sum of squared error** (depends on units, difficult to interpret)\n- $R^2$ (independent of units, easy to interpret)\n\n:::\n::::\n\n- $R^2$ quantifies the percentage of **variance** in the response variable that is explained by the model.\n\n## Variance {.smaller}\n\n$\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{n-1}$\n\n- We take the **sum of squares**\n- square the residuals ($i^{th}$ data point minus the $i^{th}$ model value)\n- then divide by the number of cases, $n$, minus 1. \n\n\n\n## Coefficient of determination, $R^2$ {.smaller}\n\n```{r}\n#| echo: false \n#| layout-ncol: 2\n\nref_model = lm(RTlexdec ~ 1, data = young_nouns_sample)\nmodel = lm(RTlexdec ~ 1 + WrittenFrequency, data = young_nouns_sample)\n\nyoung_nouns_sample <- young_nouns_sample %>% spread_predictions(ref_model, model)\n\nSSE <- function(data, par) {\n  young_nouns_sample %>%\n    mutate(prediction = par[1] + par[2]* WrittenFrequency) %>%\n    mutate(error = prediction - RTlexdec) %>%\n    mutate(squared_error = error^2) %>%\n    with(sum(squared_error))\n}\n\n\nsse_ref <- SSE(data = young_nouns_sample, par = c(6.447, 0))\nsse_model <- SSE(data = young_nouns_sample, par = c(6.59064, -0.02874))\n\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ 1\", se = FALSE) +\n    geom_segment(aes(xend = WrittenFrequency, yend = ref_model)) +\n    labs(title = \"Total variance\", subtitle = \"SSE for the 'reference' model\", caption = sse_ref)\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) + \n        geom_segment(aes(xend = WrittenFrequency, yend = model)) +\n    labs(title = \"Unexplained variance\", subtitle = \"SSE for our model\", caption = sse_model)\n\n\n```\n\n. . . \n$R^2=100\\times(1-\\frac{unexplained \\; variance}{total \\; variance})$\n\n. . . \n\n$R^2=100\\times(1-\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{\\sum_{i=1}^n (y_i - \\overline{y})^2})$\n\n. . . \n\n\n$R^2=100\\times(1-\\frac{SSE_{model}}{SSE_{reference}})$\n\n## Coefficient of determination, $R^2$ {.smaller}\n\n$R^2=100\\times(1-\\frac{SSE_{model}}{SSE_{reference}})$\n\n. . . \n\n```{r}\n\n# compute R2 from SSEs\n1 - (sse_model/sse_ref)\n```\n\n. . .\n\n```{r}\n\n# compute R2 from lm\nsummary(model)\n```\n\n## $R^2$ overestimates model accuracy {.smaller}\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n```{r}\n#| echo: false\n#| layout-nrow: 2\n\n# tmp <- lm(RTlexdec ~ WrittenFrequency + LengthInLetters, data = young_nouns)\n# tms <- lm(RTlexdec ~ WrittenFrequency + LengthInLetters, data = young_nouns_sample)\n# fmp <- lm(RTlexdec ~ WrittenFrequency, data = young_nouns)\n# fms <- lm(RTlexdec ~ WrittenFrequency, data = young_nouns_sample)\n\n# tmp \n# tms\n# fmp\n# fms\n\n# young_nouns <- young_nouns %>% spread_predictions(tmp, tms, fmp, fms)\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_point(data = young_nouns, alpha = 0.25) +\n    geom_smooth(data = young_nouns, method = \"lm\", \n        formula = \"y ~ x\", se = FALSE, color = \"black\", linewidth = 2  )  +\n        labs(title = \"True model\")\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", \n        se = FALSE, linewidth = 2) + \n    geom_point(data = young_nouns, alpha = 0.25)  +\n    labs(title = \"Fitted model\")\n\n\n```\n\n::: \n::: {.column width=\"50%\"}\n\n| | Population | Sample |\n| --- | --- | --- | \n| True  | high | high |\n| Fitted | low | `very high` |  \n\n\n\n- Accuracy of the fitted model on the sample `overestimates true accuracy` of fitted model.\n\n:::\n::::\n\n\n\n## Overfitting {.smaller}\n\nYou have the freedom to fit your sample data better and better (you can add more and more terms, increasing the $R^2$ value). But be careful not to fit the sample data *too* well. \n\n- any given set of data contains not only the `true model` (`signal`), but also random variation (`noise`). \n- Fitting the sample data too well means we fit not only the signal but also the noise in the data. \n- An overfit model will perform really well on the data it has been trained on (the sample), but would predict new, unseen values poorly. \n- Our goal is to find the `optimal fitted model` -- the one that gets as close to the true model as possible without overfitting. \n\n\n## Cross-validation justificaiton {.smaller}\n\n- We want to know: *how well does the model we fit describe the population we are interested in*. \n- But we only have the sample, and $R^2$ on the sample will tend to overestimate the model's accuracy on the population.\n- To estimate the accuracy of the model on the population, we can use `cross-validation`\n\n## Cross-validation steps {.smaller}\n\nGiven a sample of data, there are 3 simple steps to any cross-validation technique:\n\n1. Leave some data out\n2. Fit a model (to the data kept in)\n3. Evaluate the model on the left out data (e.g. $R^2$)\n\n. . . \n\nThere are many ways to do cross-validation — reflecting that there are many ways we can leave some data out — but they all follow this general 3-step process.\n\n## Two common cross-validation approaches {.smaller}\n\n- In `leave-one-out cross-validation`, we leave out a single data point and use the fitted model to predict that single point. We repeat this process for every data point, then evaluate each model's prediction on the left out points (we can use $R^2$!). \n- In `k-fold cross-validation`, instead of leaving out a single data point, we randomly divide the dataset into $k$ parts and use the fitted model to predict that *part*. We repeat this process for every part, then evaluate each model's prediction on the left out parts (again, we can use $R^2$!). \n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo13.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo12.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo11.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo10.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo9.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo8.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo7.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo5.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo4.png)\n\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo3.png)\n\n\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo2.png)\n\n\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo1.png)\n\n\n\n","srcMarkdownNoYaml":"\n\n```{r}\n#| echo: false\n#| message: false\nlibrary(tidyverse)\nlibrary(modelr)\nlibrary(infer)\nlibrary(knitr)\ntheme_set(theme_classic(base_size = 20))\nset.seed(60)\n\n# setup data \ndata <- tibble(\n    x = c(1, 2, 3, 4, 5),\n    y = c(1.2, 2.5, 2.3, 3.1, 4.4)\n)\n\ndata2 <- tibble(\n    x = c(1, 2, 3, 4, 5),\n    y = c(2, 2.5, 3.3, 4.1, 6.4)\n\n\n)\n\n```\n\n\n## You are not alone\n\n![by Allison Horst](/include/figures/Rlearning.png)\n\n## You are `here` {.smaller} \n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n##### Data science with R \n::: {.nonincremental}\n- Hello, world!\n- R basics\n- Data importing\n- Data visualization\n- Data wrangling \n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### Stats & Model buidling\n::: {.nonincremental}\n- Sampling distribution\n- Hypothesis testing\n- Model specification\n- Model fitting\n- `Model accuracy`\n- Model reliability\n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### More advanced \n::: {.nonincremental}\n\n- Classification\n- Feature engineering (preprocessing) \n- Inference for regression\n- Mixed-effect models\n::: \n:::\n\n::::\n\n## Model building overview {.smaller}\n\n- **Model specification**: what is the form?\n- **Model fitting**: you have the form, how do you guess the free parameters? \n- **Model accuracy**: you've estimated the parameters, how well does that model describe your data? \n- **Model reliability**: when you estimate the parameters, there is some uncertainty on them\n\n## Dataset {.smaller}\n\n```{r}\nlibrary(languageR)\nglimpse(english)\n```\n\n## Quantifying model accuracy {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n```{r}\n#| echo: false\nyoung_nouns <- filter(english, WordCategory == 'N', AgeSubject == \"young\")\n\nyoung_nouns_sample <- young_nouns %>% slice_sample(n = 20)\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) +\n    labs(caption = \"sample of 20 participants\")\n```\n\n::: \n::: {.column width=\"40%\"}\n\n- We can visualize to get a sense of accuracy\n- But want to `quantify` accuracy (determine whether model is useful or how it compares to other models)\n\n:::\n::::\n\n\n\n## Quantifying model accuracy {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n```{r}\n#| echo: false\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE)\n```\n\n::: \n::: {.column width=\"40%\"}\n\n- **sum of squared error** (depends on units, difficult to interpret)\n- $R^2$ (independent of units, easy to interpret)\n\n:::\n::::\n\n- $R^2$ quantifies the percentage of **variance** in the response variable that is explained by the model.\n\n## Variance {.smaller}\n\n$\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{n-1}$\n\n- We take the **sum of squares**\n- square the residuals ($i^{th}$ data point minus the $i^{th}$ model value)\n- then divide by the number of cases, $n$, minus 1. \n\n\n\n## Coefficient of determination, $R^2$ {.smaller}\n\n```{r}\n#| echo: false \n#| layout-ncol: 2\n\nref_model = lm(RTlexdec ~ 1, data = young_nouns_sample)\nmodel = lm(RTlexdec ~ 1 + WrittenFrequency, data = young_nouns_sample)\n\nyoung_nouns_sample <- young_nouns_sample %>% spread_predictions(ref_model, model)\n\nSSE <- function(data, par) {\n  young_nouns_sample %>%\n    mutate(prediction = par[1] + par[2]* WrittenFrequency) %>%\n    mutate(error = prediction - RTlexdec) %>%\n    mutate(squared_error = error^2) %>%\n    with(sum(squared_error))\n}\n\n\nsse_ref <- SSE(data = young_nouns_sample, par = c(6.447, 0))\nsse_model <- SSE(data = young_nouns_sample, par = c(6.59064, -0.02874))\n\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ 1\", se = FALSE) +\n    geom_segment(aes(xend = WrittenFrequency, yend = ref_model)) +\n    labs(title = \"Total variance\", subtitle = \"SSE for the 'reference' model\", caption = sse_ref)\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", se = FALSE) + \n        geom_segment(aes(xend = WrittenFrequency, yend = model)) +\n    labs(title = \"Unexplained variance\", subtitle = \"SSE for our model\", caption = sse_model)\n\n\n```\n\n. . . \n$R^2=100\\times(1-\\frac{unexplained \\; variance}{total \\; variance})$\n\n. . . \n\n$R^2=100\\times(1-\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{\\sum_{i=1}^n (y_i - \\overline{y})^2})$\n\n. . . \n\n\n$R^2=100\\times(1-\\frac{SSE_{model}}{SSE_{reference}})$\n\n## Coefficient of determination, $R^2$ {.smaller}\n\n$R^2=100\\times(1-\\frac{SSE_{model}}{SSE_{reference}})$\n\n. . . \n\n```{r}\n\n# compute R2 from SSEs\n1 - (sse_model/sse_ref)\n```\n\n. . .\n\n```{r}\n\n# compute R2 from lm\nsummary(model)\n```\n\n## $R^2$ overestimates model accuracy {.smaller}\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n```{r}\n#| echo: false\n#| layout-nrow: 2\n\n# tmp <- lm(RTlexdec ~ WrittenFrequency + LengthInLetters, data = young_nouns)\n# tms <- lm(RTlexdec ~ WrittenFrequency + LengthInLetters, data = young_nouns_sample)\n# fmp <- lm(RTlexdec ~ WrittenFrequency, data = young_nouns)\n# fms <- lm(RTlexdec ~ WrittenFrequency, data = young_nouns_sample)\n\n# tmp \n# tms\n# fmp\n# fms\n\n# young_nouns <- young_nouns %>% spread_predictions(tmp, tms, fmp, fms)\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_point(data = young_nouns, alpha = 0.25) +\n    geom_smooth(data = young_nouns, method = \"lm\", \n        formula = \"y ~ x\", se = FALSE, color = \"black\", linewidth = 2  )  +\n        labs(title = \"True model\")\n\nggplot(young_nouns_sample, aes(y = RTlexdec, x = WrittenFrequency)) +\n    geom_point(color = \"blue\", size = 4) +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\", \n        se = FALSE, linewidth = 2) + \n    geom_point(data = young_nouns, alpha = 0.25)  +\n    labs(title = \"Fitted model\")\n\n\n```\n\n::: \n::: {.column width=\"50%\"}\n\n| | Population | Sample |\n| --- | --- | --- | \n| True  | high | high |\n| Fitted | low | `very high` |  \n\n\n\n- Accuracy of the fitted model on the sample `overestimates true accuracy` of fitted model.\n\n:::\n::::\n\n\n\n## Overfitting {.smaller}\n\nYou have the freedom to fit your sample data better and better (you can add more and more terms, increasing the $R^2$ value). But be careful not to fit the sample data *too* well. \n\n- any given set of data contains not only the `true model` (`signal`), but also random variation (`noise`). \n- Fitting the sample data too well means we fit not only the signal but also the noise in the data. \n- An overfit model will perform really well on the data it has been trained on (the sample), but would predict new, unseen values poorly. \n- Our goal is to find the `optimal fitted model` -- the one that gets as close to the true model as possible without overfitting. \n\n\n## Cross-validation justificaiton {.smaller}\n\n- We want to know: *how well does the model we fit describe the population we are interested in*. \n- But we only have the sample, and $R^2$ on the sample will tend to overestimate the model's accuracy on the population.\n- To estimate the accuracy of the model on the population, we can use `cross-validation`\n\n## Cross-validation steps {.smaller}\n\nGiven a sample of data, there are 3 simple steps to any cross-validation technique:\n\n1. Leave some data out\n2. Fit a model (to the data kept in)\n3. Evaluate the model on the left out data (e.g. $R^2$)\n\n. . . \n\nThere are many ways to do cross-validation — reflecting that there are many ways we can leave some data out — but they all follow this general 3-step process.\n\n## Two common cross-validation approaches {.smaller}\n\n- In `leave-one-out cross-validation`, we leave out a single data point and use the fitted model to predict that single point. We repeat this process for every data point, then evaluate each model's prediction on the left out points (we can use $R^2$!). \n- In `k-fold cross-validation`, instead of leaving out a single data point, we randomly divide the dataset into $k$ parts and use the fitted model to predict that *part*. We repeat this process for every part, then evaluate each model's prediction on the left out parts (again, we can use $R^2$!). \n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo13.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo12.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo11.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo10.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo9.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo8.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo7.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo5.png)\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo4.png)\n\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo3.png)\n\n\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo2.png)\n\n\n\n## Leave-one-out cross-validation {.smaller}\n\n![Figure borrowed from Kendrick Kay](/include/figures/loo1.png)\n\n\n\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","incremental":true,"output-file":"model-accuracy.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.3.353","auto-stretch":true,"callout-appearance":"simple","title":"Model Accuracy","subtitle":"Data Science for Studying Language and the Mind","author":"Katie Schuler","date":"10-24-2023","theme":"dark","slideNumber":true,"footer":"[https://kathrynschuler.com/datasci](https://kathrynschuler.com/datasci/)"}}},"projectFormats":["html"]}