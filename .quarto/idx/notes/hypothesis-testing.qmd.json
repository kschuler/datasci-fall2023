{"title":"Hypothesis testing","markdown":{"yaml":{"title":"Hypothesis testing","date":"2023-09-28","author":["Katie Schuler"]},"headingText":"load the packages we need","containsRefs":false,"markdown":"\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Setup code\"\n#| message: false\n\nlibrary(tidyverse)\nlibrary(infer)\n\n# set the theme of all figures\ntheme_set(theme_classic(base_size = 14))\n\n# set a seed so we get the same random stuff every time\nset.seed(60)\n\n\n# generate some data for female penn students\nfemales <- tibble(\n    volume = rnorm(5625, mean = 1200, sd = 92),\n    sex = \"female\"\n)\n\n# generate some data for male penn students\nmales <- tibble(\n    volume = rnorm(5625, mean = 1220, sd = 98),\n    sex = \"male\"\n)\n\n# make the whole population \npenn_pop <- bind_rows(males, females)\n\n# sample 200 participants from this population\npenn_sample <- penn_pop %>% slice_sample(n = 200)\n\n# get just the female and just the male participants\n# to use in figures \npenn_sample_f <- filter(penn_sample, sex == \"female\")\npenn_sample_m <- filter(penn_sample, sex == \"male\")\n\n# compute the mean to use in some figures \nmean_f <- mean(penn_sample_f$volume)\nmean_m <- mean(penn_sample_m$volume)\n\n```\n\nLast week we explored a simple dataset in which we measured brain volume for a sample of Penn students and computed the mean. Suppose we now want to take into account the sex of the students. What can we do with these data? \n\nThe first thing we should do is visualize our data. A useful visualization for a categorical variable is a boxplot: \n\n```{r}\npenn_sample %>%\n  ggplot(aes(y = volume, x = sex)) +\n  geom_boxplot(aes(color = sex))\n```\n\nOne thing we might want to ask is whether the two sexes differ in mean brain volume. Let's compute the difference we observe in our sample with `infer`: \n\n```{r}\nobs_diff <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\nobs_diff\n```\n\nLast week we saw that the mean of any sample drawn from the population will be subject to sampling variability. The same situation applies here: the difference in means will also differ depending on our sample. Thus, even if we find a difference in means, that difference could be due to sampling error (not any true difference in the population).\n\n\n## Hypothesis testing framework\n\nTo determine whether the brains of male and female Penn students differ with respect to the mean, we can use a framework for decision making called **hypothesis testing**. We can think of this as a 3-step process:\n\n1. First we pose a **null hypothesis** that our result is due to nothing but chance (sampling variability)\n2. Then we ask: if the null hypothesis is true, how likely is our observed pattern of results? This liklihood is the **p-value**. \n3. Finally, if the p-value is less some threshold we decide upon (<0.05) then we **reject** the null hypothesis.\n\n### Step 1: Pose the null hypothesis\nWe pose a null hypothesis for practical reasons: it is the hypothesis for which we can simulate data. We can construct the sampling distribution for a hypothetical world in which our observed value is due to chance (we call this the **null distribution**). \n\n- To construct the null distribution we can use a process called **randomization**. \n- Randomization is similar to bootstrapping except, on each repeat, we will shuffle the sex and randomly assigning it to each participant. \n- This simulates a world in which there is no relationship between brain volume and sex. \n\n<!-- We pose a null hypothesis for practical reasons. We want to avoid reasoning about the real world (full of unknowns!) and instead construct a hypothetical world where we can assume the hypothesis is true and draw out the consequences of that assumption with deductive reasoning:  -->\n\n<!-- > If hypothesis $H$ is true, then the test statistic $S$ will be drawn from probability distribution $P$\n\n- Our **test statistic $S$** is the observed difference in means between males and females in our sample of Penn students. \n- Our **null hypothesis $H$** is that the difference in means is due to chance (nothing more than sampling variability). \n- Our **probability distribution $P$** is the sampling distribution for the test statistic (here the difference in means) in our hypothetical null hypothesis world where the difference is due to nothing but chance (sampling variability)! This sampling distribution has a special name, the **null distribution**, and has the practical advantage that it is something we can simulate with our data. -->\n\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: Show code\n#| layout-ncol: 2\n\n\nnull_distribution <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) \n\nnull_distribution %>% \n  visualize() + \n  labs(x = \"stat (difference in means)\")\n```\n\n\n### Step 2: How likely is our observed pattern?\n\nIf the null hypothesis is true, how likely is our observed pattern of results? We can quantify this liklihood directly with the p-value: count the number of values in our null distribution that are more extreme than our observed value and divide that by the number of simulations we generated. \n\n```{r}\n\nnull_distribution %>% \n  filter(abs(stat) > obs_diff$stat) %>%\n  summarise(p = n()/1000)\n```\n\nOr `infer` can handle this for us with the `get_p_value()` function:\n\n```{r}\nnull_distribution %>%\n  get_p_value(obs_stat = obs_diff, direction = \"both\")\n```\n\nA large p-value means our observed value is very likely under the null hypotheisis. A small p-value means our observed value is very unlikely under the null hypothsis. \n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: Show code\n#| layout-ncol: 2\n\n\n\nobs_diff <- penn_sample %>% \n  observe(\n    volume ~ sex,\n    stat = \"diff in means\",\n    order = c(\"male\", \"female\")\n  )\n\nnull_distribution %>% \n  visualize() + \n  shade_p_value(obs_stat = obs_diff, direction = \"two-sided\") +\n  labs(x = \"stat (difference in means)\")\n\n```\n\n### Step 3: Decide whether to reject the null\n\nFinally, if the p-value is small enough — less than some threshold we decide upon — we reject the null hypothesis. By convention we consider a p-value less than 0.05 to be implausible enough that we can reject the null hypothesis (read more about [why 0.05](https://inferentialthinking.com/chapters/11/3/Decisions_and_Uncertainty.html#historical-note-on-the-conventions). Note that obtaining our observed value is *implausible* under the null, but not *impossible*. In other words, our decision to reject (or not) could be wrong!\n\n- When we reject a null hypothesis that was actually true, we call it a **type I error**.\n- When we fail to reject a null hypothesis that was actually false, we call it a **type II error** \n\n\n![This figure borrowed from reddit can help you remember the error types. (Null hypothesis here is \"NOT pregnant\")](/include/figures/type1-type2-errors.png)\n\n::: {.callout-tip collapse=\"true\"}\n## Do you know the \"The boy who cried wolf\" story?\n\nRemembering which error is I or II can be tricky. June likes to analogize it to \"The boy who cried wolf\" to remember errors in chronological order:\n\n- **Type I** error is the *first* thing that happens: Townspeople think there is a wolf, but it's not actually there! (**false positive** - wrongly thinking that the effect is *present*)\n- **Type II** error is the *second* thing that happens: The wolf actually appears but townspeople don't believe it! (**false negative** - wrongly thinking that the effect is *absent*)\n:::\n\n## There is only one test\n\n![Figure borrowed from Modern Dive textbook](/include/figures/one-test.png){fig-align=\"center\" width=50%}\n\nComputer scientist Allen Downey famously dubbed the framework outlined above the “There is only one test” framework. It allows us to appreciate that, though there are a myriad of statistical tests available, there is really only one hypothesis test. If you understand this general framework, you can understand any hypothesis test (t-test, chi-squared, etc). \n\nRemember from last week that there are two ways we can construct a sampling distribution (simulate data):\n\n1. **Nonparametrically**, via brute computational force (simulating many repeats of the same experiment with bootstrapping or randomization) \n2. **Parametrically**, by assuming the data were sampled from known probability distribution and working out what happens theoretically under that distribution. \n\nIn the approach above, I've demonstrated the nonparametric way. But we could have opted for a parametric test instead. Let's illustrate with a t-test. \n\n### t-test example\n\nWe can use R's built in function to perform a t-test to compare two means.\n\n```{r}\nt.test(penn_sample_m$volume, penn_sample_f$volume)\n```\n\nUnder the hood, the `t.test()` is performing the same \"one test\", it is just constructing the null distribution parametrically (assuming a known probability distribution defined by parameters). We can see this with `infer` by asking it to `assume()` a distribution rather than `generate()` data.\n\n```{r}\n# calculate the observed value (t)\nobs_val <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  calculate(stat = \"t\",order = c(\"male\", \"female\")) \n\nobs_val\n```\n\n```{r}\n# assume the t distribution \nnull_distribution_theo <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  assume(distribution =\"t\") \n\n# return the p value \nnull_distribution_theo %>%\n    get_p_value(obs_val, direction = \"both\")\n\n```\n\n```{r}\n#| layout-ncol: 2\n\n# visualize the theoretical distribution\nnull_distribution_theo %>%\n  visualize() +\n  shade_p_value(obs_val, direction = \"both\")\n\n```\n\n\n## Exploring relationships \n\nSo far we explored data in which we measured a single quantity: brain volume. Suppose we have a slightly more complex dataset in which we measure two quantities! Let's use the `ratings` data from the `languageR` package. \n\n```{r}\nlibrary(languageR)\n```\n\nWe might want to know whether there is a relationship between two variables. We can explore the relationship between two quantities visually with a **scatter plot**.\n\n```{r}\n#| layout-ncol: 2\nratings %>% \n  ggplot(aes(x = Frequency, y = meanFamiliarity)) +\n  geom_point()\n\n```\n\n\nIf there is no relationship between the variables, we say they are **independent**. We can think of independence in the following way: knowing the value of one variable provides no information about the other variable. In the `ratings` data, that would mean the word's actual frequency provides provides no information about the participants mean familiarity rating. If there *is* some relationship between the variables, we can consider two types:\n\n1. There may be a **linear relationship** between the variables. When one goes up the other goes up (positive) or when one goes up the other goes down (negative). In our example, there is a linear relationship between meanFamiliairity and Frequency: as Frequency increases, meanFamiliarity also increases. \n2. Or a **nonlinear relationship**. Nonlinear is a very broad category that encompasses all relationships that are not linear (e.g. a U-shaped curve).\n\n\n\n## Correlation \n\nOne way to quantify linear relationships is with **correlation ($r$)**. Correlation expresses the linear relationship as a range from -1 to 1, where -1 means the relationship is perfectly negative and 1 means the relationship is perfectly positive. \n\n![Figure borrowed from iStock photos](/include/figures/correlation.jpeg){fig-align=\"center\" width=50%}\n\nCorrelation can be calculated by taking the z-score of each variable (a normalization technique in which we subtract the mean and divide by the standard deviation) and then computing the average product of each variable: \n\n$r=\\frac{\\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{sd(x)})(\\frac{y_i - \\bar{y}}{sd(y)})}{n}$\n\nOr we can use R's built in correlation function: `cor(x,y)`\n\n```{r}\ncor(ratings$Frequency, ratings$meanFamiliarity)\n```\n\nJust like the mean — and all other test statistics! — *$r$* is subject to sampling variability. We can indicate our uncertainty around the correlation we observe in the same way we did for the mean last week: construct the sampling distribution of the correlation via bootstrapping and compute a confidence interval. \n\n```{r}\n# get the observed correlation \nobs_corr <- ratings %>%\n  specify(response = meanFamiliarity, explanatory = Frequency) %>%\n  calculate(stat = \"correlation\")\n\nobs_corr\n```\n\n```{r}\n# construct the sampling distribution\nsamp_dist <- ratings %>%\n  specify(response = meanFamiliarity, explanatory = Frequency) %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  calculate(stat = \"correlation\")\n```\n\n```{r}\n# compute a confidence interval\nci <- samp_dist %>%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nci\n```\n\n```{r}\n#| layout-ncol: 2\n\n# visualize the distribution \nsamp_dist %>% \n  visualize() +\n  shade_ci(ci)\n\n```\n\nHow do we test whether the correlation we observed in the data is significantly different from zero? We can use hypothesis testing (as learned today)! Here our null hypothesis that there is no relationship between the variables (they are **independent**).  First we generate the null distribution:\n\n\n```{r}\n#| layout-ncol: 2\n\n# generate the null distribution \nnull_distribution_corr <- ratings %>% \n  specify(response = meanFamiliarity, explanatory = Frequency) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"correlation\") \n\nnull_distribution_corr %>% \n  visualize() \n```\n\nThen we ask how likily our observed correlation is under the null hypothesis (get a p-value): \n\n```{r}\n# get the p-value\nnull_distribution_corr %>%\n  get_p_value(obs_stat = obs_corr, direction = \"both\")\n```\n\n```{r}\n#| layout-ncol: 2\n\n# visualize p-value on the null distribution\nnull_distribution_corr %>% \n  visualize() + \n  shade_p_value(obs_stat = obs_corr, direction = \"two-sided\") \n```\n\nFinally we decide whether to reject the null hypothesis if the liklihood of observing this correlation under the null is small enough (<0.05). We can see that our observed correlation is very unlikely in the null distribution  (p-value = 0), so we reject the null hypothesis. \n\n\n## Further reading\n\n- [The logic of hypothesis testing](https://dtkaplan.github.io/SM2-bookdown/the-logic-of-hypothesis-testing.html) - Chapter 13, Statistical Modeling\n- [Hypothesis testing](https://moderndive.com/9-hypothesis-testing.html) - Chapter 9, Modern Dive\n- [Decisions and Uncertainty](https://inferentialthinking.com/chapters/11/3/Decisions_and_Uncertainty.html) - Chapter 11 Section 3 Inferential Thinking\n","srcMarkdownNoYaml":"\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Setup code\"\n#| message: false\n\n# load the packages we need\nlibrary(tidyverse)\nlibrary(infer)\n\n# set the theme of all figures\ntheme_set(theme_classic(base_size = 14))\n\n# set a seed so we get the same random stuff every time\nset.seed(60)\n\n\n# generate some data for female penn students\nfemales <- tibble(\n    volume = rnorm(5625, mean = 1200, sd = 92),\n    sex = \"female\"\n)\n\n# generate some data for male penn students\nmales <- tibble(\n    volume = rnorm(5625, mean = 1220, sd = 98),\n    sex = \"male\"\n)\n\n# make the whole population \npenn_pop <- bind_rows(males, females)\n\n# sample 200 participants from this population\npenn_sample <- penn_pop %>% slice_sample(n = 200)\n\n# get just the female and just the male participants\n# to use in figures \npenn_sample_f <- filter(penn_sample, sex == \"female\")\npenn_sample_m <- filter(penn_sample, sex == \"male\")\n\n# compute the mean to use in some figures \nmean_f <- mean(penn_sample_f$volume)\nmean_m <- mean(penn_sample_m$volume)\n\n```\n\nLast week we explored a simple dataset in which we measured brain volume for a sample of Penn students and computed the mean. Suppose we now want to take into account the sex of the students. What can we do with these data? \n\nThe first thing we should do is visualize our data. A useful visualization for a categorical variable is a boxplot: \n\n```{r}\npenn_sample %>%\n  ggplot(aes(y = volume, x = sex)) +\n  geom_boxplot(aes(color = sex))\n```\n\nOne thing we might want to ask is whether the two sexes differ in mean brain volume. Let's compute the difference we observe in our sample with `infer`: \n\n```{r}\nobs_diff <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\"))\n\nobs_diff\n```\n\nLast week we saw that the mean of any sample drawn from the population will be subject to sampling variability. The same situation applies here: the difference in means will also differ depending on our sample. Thus, even if we find a difference in means, that difference could be due to sampling error (not any true difference in the population).\n\n\n## Hypothesis testing framework\n\nTo determine whether the brains of male and female Penn students differ with respect to the mean, we can use a framework for decision making called **hypothesis testing**. We can think of this as a 3-step process:\n\n1. First we pose a **null hypothesis** that our result is due to nothing but chance (sampling variability)\n2. Then we ask: if the null hypothesis is true, how likely is our observed pattern of results? This liklihood is the **p-value**. \n3. Finally, if the p-value is less some threshold we decide upon (<0.05) then we **reject** the null hypothesis.\n\n### Step 1: Pose the null hypothesis\nWe pose a null hypothesis for practical reasons: it is the hypothesis for which we can simulate data. We can construct the sampling distribution for a hypothetical world in which our observed value is due to chance (we call this the **null distribution**). \n\n- To construct the null distribution we can use a process called **randomization**. \n- Randomization is similar to bootstrapping except, on each repeat, we will shuffle the sex and randomly assigning it to each participant. \n- This simulates a world in which there is no relationship between brain volume and sex. \n\n<!-- We pose a null hypothesis for practical reasons. We want to avoid reasoning about the real world (full of unknowns!) and instead construct a hypothetical world where we can assume the hypothesis is true and draw out the consequences of that assumption with deductive reasoning:  -->\n\n<!-- > If hypothesis $H$ is true, then the test statistic $S$ will be drawn from probability distribution $P$\n\n- Our **test statistic $S$** is the observed difference in means between males and females in our sample of Penn students. \n- Our **null hypothesis $H$** is that the difference in means is due to chance (nothing more than sampling variability). \n- Our **probability distribution $P$** is the sampling distribution for the test statistic (here the difference in means) in our hypothetical null hypothesis world where the difference is due to nothing but chance (sampling variability)! This sampling distribution has a special name, the **null distribution**, and has the practical advantage that it is something we can simulate with our data. -->\n\n\n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: Show code\n#| layout-ncol: 2\n\n\nnull_distribution <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) \n\nnull_distribution %>% \n  visualize() + \n  labs(x = \"stat (difference in means)\")\n```\n\n\n### Step 2: How likely is our observed pattern?\n\nIf the null hypothesis is true, how likely is our observed pattern of results? We can quantify this liklihood directly with the p-value: count the number of values in our null distribution that are more extreme than our observed value and divide that by the number of simulations we generated. \n\n```{r}\n\nnull_distribution %>% \n  filter(abs(stat) > obs_diff$stat) %>%\n  summarise(p = n()/1000)\n```\n\nOr `infer` can handle this for us with the `get_p_value()` function:\n\n```{r}\nnull_distribution %>%\n  get_p_value(obs_stat = obs_diff, direction = \"both\")\n```\n\nA large p-value means our observed value is very likely under the null hypotheisis. A small p-value means our observed value is very unlikely under the null hypothsis. \n\n```{r}\n#| echo: true\n#| code-fold: true\n#| code-summary: Show code\n#| layout-ncol: 2\n\n\n\nobs_diff <- penn_sample %>% \n  observe(\n    volume ~ sex,\n    stat = \"diff in means\",\n    order = c(\"male\", \"female\")\n  )\n\nnull_distribution %>% \n  visualize() + \n  shade_p_value(obs_stat = obs_diff, direction = \"two-sided\") +\n  labs(x = \"stat (difference in means)\")\n\n```\n\n### Step 3: Decide whether to reject the null\n\nFinally, if the p-value is small enough — less than some threshold we decide upon — we reject the null hypothesis. By convention we consider a p-value less than 0.05 to be implausible enough that we can reject the null hypothesis (read more about [why 0.05](https://inferentialthinking.com/chapters/11/3/Decisions_and_Uncertainty.html#historical-note-on-the-conventions). Note that obtaining our observed value is *implausible* under the null, but not *impossible*. In other words, our decision to reject (or not) could be wrong!\n\n- When we reject a null hypothesis that was actually true, we call it a **type I error**.\n- When we fail to reject a null hypothesis that was actually false, we call it a **type II error** \n\n\n![This figure borrowed from reddit can help you remember the error types. (Null hypothesis here is \"NOT pregnant\")](/include/figures/type1-type2-errors.png)\n\n::: {.callout-tip collapse=\"true\"}\n## Do you know the \"The boy who cried wolf\" story?\n\nRemembering which error is I or II can be tricky. June likes to analogize it to \"The boy who cried wolf\" to remember errors in chronological order:\n\n- **Type I** error is the *first* thing that happens: Townspeople think there is a wolf, but it's not actually there! (**false positive** - wrongly thinking that the effect is *present*)\n- **Type II** error is the *second* thing that happens: The wolf actually appears but townspeople don't believe it! (**false negative** - wrongly thinking that the effect is *absent*)\n:::\n\n## There is only one test\n\n![Figure borrowed from Modern Dive textbook](/include/figures/one-test.png){fig-align=\"center\" width=50%}\n\nComputer scientist Allen Downey famously dubbed the framework outlined above the “There is only one test” framework. It allows us to appreciate that, though there are a myriad of statistical tests available, there is really only one hypothesis test. If you understand this general framework, you can understand any hypothesis test (t-test, chi-squared, etc). \n\nRemember from last week that there are two ways we can construct a sampling distribution (simulate data):\n\n1. **Nonparametrically**, via brute computational force (simulating many repeats of the same experiment with bootstrapping or randomization) \n2. **Parametrically**, by assuming the data were sampled from known probability distribution and working out what happens theoretically under that distribution. \n\nIn the approach above, I've demonstrated the nonparametric way. But we could have opted for a parametric test instead. Let's illustrate with a t-test. \n\n### t-test example\n\nWe can use R's built in function to perform a t-test to compare two means.\n\n```{r}\nt.test(penn_sample_m$volume, penn_sample_f$volume)\n```\n\nUnder the hood, the `t.test()` is performing the same \"one test\", it is just constructing the null distribution parametrically (assuming a known probability distribution defined by parameters). We can see this with `infer` by asking it to `assume()` a distribution rather than `generate()` data.\n\n```{r}\n# calculate the observed value (t)\nobs_val <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  calculate(stat = \"t\",order = c(\"male\", \"female\")) \n\nobs_val\n```\n\n```{r}\n# assume the t distribution \nnull_distribution_theo <- penn_sample %>% \n  specify(response = volume, explanatory = sex) %>%\n  assume(distribution =\"t\") \n\n# return the p value \nnull_distribution_theo %>%\n    get_p_value(obs_val, direction = \"both\")\n\n```\n\n```{r}\n#| layout-ncol: 2\n\n# visualize the theoretical distribution\nnull_distribution_theo %>%\n  visualize() +\n  shade_p_value(obs_val, direction = \"both\")\n\n```\n\n\n## Exploring relationships \n\nSo far we explored data in which we measured a single quantity: brain volume. Suppose we have a slightly more complex dataset in which we measure two quantities! Let's use the `ratings` data from the `languageR` package. \n\n```{r}\nlibrary(languageR)\n```\n\nWe might want to know whether there is a relationship between two variables. We can explore the relationship between two quantities visually with a **scatter plot**.\n\n```{r}\n#| layout-ncol: 2\nratings %>% \n  ggplot(aes(x = Frequency, y = meanFamiliarity)) +\n  geom_point()\n\n```\n\n\nIf there is no relationship between the variables, we say they are **independent**. We can think of independence in the following way: knowing the value of one variable provides no information about the other variable. In the `ratings` data, that would mean the word's actual frequency provides provides no information about the participants mean familiarity rating. If there *is* some relationship between the variables, we can consider two types:\n\n1. There may be a **linear relationship** between the variables. When one goes up the other goes up (positive) or when one goes up the other goes down (negative). In our example, there is a linear relationship between meanFamiliairity and Frequency: as Frequency increases, meanFamiliarity also increases. \n2. Or a **nonlinear relationship**. Nonlinear is a very broad category that encompasses all relationships that are not linear (e.g. a U-shaped curve).\n\n\n\n## Correlation \n\nOne way to quantify linear relationships is with **correlation ($r$)**. Correlation expresses the linear relationship as a range from -1 to 1, where -1 means the relationship is perfectly negative and 1 means the relationship is perfectly positive. \n\n![Figure borrowed from iStock photos](/include/figures/correlation.jpeg){fig-align=\"center\" width=50%}\n\nCorrelation can be calculated by taking the z-score of each variable (a normalization technique in which we subtract the mean and divide by the standard deviation) and then computing the average product of each variable: \n\n$r=\\frac{\\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{sd(x)})(\\frac{y_i - \\bar{y}}{sd(y)})}{n}$\n\nOr we can use R's built in correlation function: `cor(x,y)`\n\n```{r}\ncor(ratings$Frequency, ratings$meanFamiliarity)\n```\n\nJust like the mean — and all other test statistics! — *$r$* is subject to sampling variability. We can indicate our uncertainty around the correlation we observe in the same way we did for the mean last week: construct the sampling distribution of the correlation via bootstrapping and compute a confidence interval. \n\n```{r}\n# get the observed correlation \nobs_corr <- ratings %>%\n  specify(response = meanFamiliarity, explanatory = Frequency) %>%\n  calculate(stat = \"correlation\")\n\nobs_corr\n```\n\n```{r}\n# construct the sampling distribution\nsamp_dist <- ratings %>%\n  specify(response = meanFamiliarity, explanatory = Frequency) %>%\n  generate(reps = 1000, type = \"bootstrap\") %>%\n  calculate(stat = \"correlation\")\n```\n\n```{r}\n# compute a confidence interval\nci <- samp_dist %>%\n  get_confidence_interval(level = 0.95, type = \"percentile\")\n\nci\n```\n\n```{r}\n#| layout-ncol: 2\n\n# visualize the distribution \nsamp_dist %>% \n  visualize() +\n  shade_ci(ci)\n\n```\n\nHow do we test whether the correlation we observed in the data is significantly different from zero? We can use hypothesis testing (as learned today)! Here our null hypothesis that there is no relationship between the variables (they are **independent**).  First we generate the null distribution:\n\n\n```{r}\n#| layout-ncol: 2\n\n# generate the null distribution \nnull_distribution_corr <- ratings %>% \n  specify(response = meanFamiliarity, explanatory = Frequency) %>%\n  hypothesize(null = \"independence\") %>%\n  generate(reps = 1000, type = \"permute\") %>%\n  calculate(stat = \"correlation\") \n\nnull_distribution_corr %>% \n  visualize() \n```\n\nThen we ask how likily our observed correlation is under the null hypothesis (get a p-value): \n\n```{r}\n# get the p-value\nnull_distribution_corr %>%\n  get_p_value(obs_stat = obs_corr, direction = \"both\")\n```\n\n```{r}\n#| layout-ncol: 2\n\n# visualize p-value on the null distribution\nnull_distribution_corr %>% \n  visualize() + \n  shade_p_value(obs_stat = obs_corr, direction = \"two-sided\") \n```\n\nFinally we decide whether to reject the null hypothesis if the liklihood of observing this correlation under the null is small enough (<0.05). We can see that our observed correlation is very unlikely in the null distribution  (p-value = 0), so we reject the null hypothesis. \n\n\n## Further reading\n\n- [The logic of hypothesis testing](https://dtkaplan.github.io/SM2-bookdown/the-logic-of-hypothesis-testing.html) - Chapter 13, Statistical Modeling\n- [Hypothesis testing](https://moderndive.com/9-hypothesis-testing.html) - Chapter 9, Modern Dive\n- [Decisions and Uncertainty](https://inferentialthinking.com/chapters/11/3/Decisions_and_Uncertainty.html) - Chapter 11 Section 3 Inferential Thinking\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css","../include/webex.css"],"toc":true,"number-sections":true,"include-in-header":["../include/webex.js"],"output-file":"hypothesis-testing.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","callout-appearance":"simple","theme":"cosmo","fontsize":"1em","linestretch":1.35,"number-depth":2,"grid":{"sidebar-width":"300px"},"title":"Hypothesis testing","date":"2023-09-28","author":["Katie Schuler"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}