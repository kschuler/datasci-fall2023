{"title":"Practice quiz 3","markdown":{"yaml":{"title":"Practice quiz 3","subtitle":"Not graded, just practice","date":"2023-10-26","author":"Katie Schuler","number-sections":false},"headingText":"Model fitting","containsRefs":false,"markdown":"\n\n```{r}\n#| echo: false\n#| message: false\n\nlibrary(webexercises)\nlibrary(tidyverse)\nlibrary(optimg)\nlibrary(tidymodels)\ntheme_set(theme_gray(base_size = 16))\nset.seed(60)\n\ndata <- tibble(\n    y = rnorm(1000, 5, 1), \n    noise = rnorm(1000, 10, 4), \n    x = y + 10 + noise\n)\n```\n\n\n1. True or false, gradient descent and orinary least squares are both iterative optimization algorithms.\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\"True\", answer=\"False\")\n\ncat(longmcq(choices))\n\n```\n\n\n2. What cost function have we been using to perform our gradient descent? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"standard deviation\",\n    \"bootstrapping\",\n    answer=\"sum of squared error\",\n    \"absolute error\"\n)\n\ncat(longmcq(choices))\n```\n\n3. True or false, when performing gradient descent on the model given by the equation $y = w_0 + w_1x_1 + w_2x_2$, we might arrive at a local minimum and miss the global one.\n\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\"True\", answer=\"False\")\n\ncat(longmcq(choices))\n\n```\n\n4. Which of the following would work to estimate the free parameters of a **nonlinear** model? \n\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer=\"gradient descent\",\n    \"ordinary least squares solution\",\n    \"both work\"\n)\n\ncat(longmcq(choices))\n```\n\n5. True or false, in gradient descent, we search through all possible parameters in the parameter space. \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\"True\", answer=\"False\")\n\ncat(longmcq(choices))\n\n```\n\n\n## Model fitting in R \n\nQuestions 6-9 refer to the code and output below, performing gradient descent with `optimg`: \n\n```{r}\n#| echo: false \n\nSSE <- function(data, par) {\n  data %>%\n    mutate(prediction = par[1] + par[2]* x) %>%\n    mutate(error = prediction - y) %>%\n    mutate(squared_error = error^2) %>%\n    with(sum(squared_error))\n}\n\n```\n\n```{r}\noptimg(data = data, par = c(0,0), fn=SSE, method = \"STGD\")\n```\n\n6. How many steps did the gradient descent algorithm take? `r fitb(6, width =10)`\n\n7. What was the sum of squared error of the optimal paramters?  `r fitb(959.4293, width =10)`\n\n8. What coefficients does the algorithm converge on? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"3.37930046, 0.06683237\",\n    \"0, 0\",\n    \"959.4293\", \n    \"6, 0\", \n    \"all of the above\"\n)\n\ncat(longmcq(choices))\n```\n\n9. What parameters were used to initialized the algorithm? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"3.37930046, 0.06683237\",\n    answer =\"0, 0\",\n    \"959.4293\", \n    \"6, 0\"\n)\n\ncat(longmcq(choices))\n```\n\nQuestions 10-12 refer to the output below from `lm()`: \n\n```{r}\n#| echo: false\nlm(y ~ x, data = data)\n\n```\n\n\n10. Use R notation to write the model specification. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"answer\"\n#| eval: false\ny ~ x  # this works (implicit intercept)\n\ny ~ 1 + x # this also works (explicit intercept)\n\n```\n\n\n11. Given the model is specified by the equation $y = w_0+w_1x_1$, what is the parameter estimate for $w_0$ = `r fitb(3.37822, width = 10)` and $w_1$ = `r fitb(0.06688, width = 10)`. \n\n\n12. True or false, for this model, `optimg()` with gradient descent would converge on the same parameter estimates? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(answer=\"True\", \"False\")\n\ncat(longmcq(choices))\n\n```\n\n\n## Model accuracy \n\nQuestion 13 refers to the following figure: \n\n```{r}\n#| echo: false\n\nset.seed(1337)\nbeta0 = 2\nbeta1 = 0.5\ndata <- tibble(x = runif(200, min = 0, max = 10),\n               y = beta0+ (beta1*x) + rnorm(200, sd = 0.5))\n\n\n```\n\n```{r}\n#| echo: false\n\nmodel <- lm(y ~ 1 + x, data = data)\nmodel_int <- lm(y ~ 1, data = data)\n\ndata <- data %>%\n    mutate(mod_pred = predict(model)) %>%\n    mutate(mod_int = predict(model_int))\n\nggplot(\n    data, aes(x = x, y = y) \n) +\n    geom_point() +\n    geom_segment(aes(xend = x, yend = mod_pred), color = \"red\") +\n    geom_smooth(method=\"lm\", se = FALSE, formula = \"y ~ x\") \n\n```\n\n\n13. In the figure above, which of the following corresponds to the residuals? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"blue line\",\n    answer =\"red lines\",\n    \"black dots\", \n    \"none of the above\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\n14. Suppose the $R^2$ value on the model in the figure above is about 0.88. Given this value, which of the following best describes the accuracy of the model? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"fairly high accuracy \",\n    \"fairly low accuracy\",\n    \"not enough information\"\n)\n\ncat(longmcq(choices))\n\n```\n\n15. Suppose 0.88 reflects the $R^2$ for our fitted model on our sample. Which of the following is true about the $R^2$ for our fitted model on the population? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"tends to be higher\",\n    answer = \"tends to be lower\",\n    \"the same\"\n)\n\ncat(longmcq(choices))\n\n```\n\n16. Which of the following best describes an overfit model? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"performs well predicting new values, but poorly on the sample\",\n    answer = \"performs well on the sample, but poorly predicting new values\",\n    \"performs poorly both on the sample and predicting new values\",\n    \"performs well both on the sample and predicting new values\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\n17. How can we estimate $R^2$ on the population? Choose all that apply.\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer=\"cross validation\",\n    answer = \"bootstrapping\",\n    \"set.seed\", \n    \"none of the above\"\n)\n\ncat(longmcq(choices))\n\n```\n\n18. Fill in the blanks below to best describe cross validation:\n    - Leave some out\n    - Fit a model on the data `r mcq(c(\"left out\", answer =  \"kept in\"))`\n    - Evaluate the mdoel on the data `r mcq(c(answer =\"left out\",\"kept in\"))` \n\n## Model accuracy in R \n\n\nQuestions 19-20 refer to the following code and output:\n\n```{r}\nmodel <- lm(y ~ 1 + x, data)\nsummary(model)\n```\n\n19. What is the $R^2$ value for the model fit above? \n\n`r fitb(0.8822, width=10)`\n\n20. Does the value you entered in 19 reflect $R^2$ on the population or on the sample? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"population\",\n    answer = \"sample\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\nQuestions 21-23 refer to the following code and output: \n\n```{r}\n# we divide the data into v folds for cross-validation\nset.seed(2) \nsplits <- vfold_cv(data)\n\n# model secification \nmodel_spec <- \n  linear_reg() %>%  \n  set_engine(engine = \"lm\")  \n\n# add a workflow\nour_workflow <- \n  workflow() %>%\n  add_model(model_spec) %>%  \n  add_formula(y ~ x) \n\n# fit models to our folds \nfitted_models <- \n  fit_resamples(\n    object = our_workflow, \n    resamples = splits\n    ) \n\nfitted_models %>%\n    collect_metrics()\n\n```\n\n21. In the cross-validation performed above, how many folds were the data split into? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"2\",\n   \"5\",\n    answer = \"10\"\n)\n\ncat(longmcq(choices))\n\n```\n\n22. What $R^2$ do we estimate for the population? \n\n`r fitb(0.890, width = 10)`\n\n23. What model has been fit? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"`y ~ x`\",\n    \"`y ~ x^2`\",\n   \"`x ~ y`\",\n    \"`vfold_cv`\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\nQuestions 24-26 refer to the following code and output: \n\n```{r}\n# we bootstrap the data for cross-validation\nset.seed(2) \nbootstrap <- bootstraps(data) \n\n# fit models to our folds \nfitted_models_boot <- \n  fit_resamples(\n    object = our_workflow, \n    resamples = bootstrap\n    ) \n\nfitted_models_boot %>%\n    collect_metrics()\n\n```\n\n24. How many bootstrap samples did we generate? \n\n`r fitb(25, width = 10)`\n\n25. True or false, we fit the same model to the bootstrap data as we did in the cross-validation code. \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"TRUE\",\n    \"FALSE\"\n)\n\ncat(longmcq(choices))\n\n```\n\n26. True or false, the $R^2$ estimated by bootstrapping is equal to the $R^2$ estimated by cross-validation.\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"TRUE\",\n    answer=\"FALSE\"\n)\n\ncat(longmcq(choices))\n\n```","srcMarkdownNoYaml":"\n\n```{r}\n#| echo: false\n#| message: false\n\nlibrary(webexercises)\nlibrary(tidyverse)\nlibrary(optimg)\nlibrary(tidymodels)\ntheme_set(theme_gray(base_size = 16))\nset.seed(60)\n\ndata <- tibble(\n    y = rnorm(1000, 5, 1), \n    noise = rnorm(1000, 10, 4), \n    x = y + 10 + noise\n)\n```\n\n## Model fitting \n\n1. True or false, gradient descent and orinary least squares are both iterative optimization algorithms.\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\"True\", answer=\"False\")\n\ncat(longmcq(choices))\n\n```\n\n\n2. What cost function have we been using to perform our gradient descent? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"standard deviation\",\n    \"bootstrapping\",\n    answer=\"sum of squared error\",\n    \"absolute error\"\n)\n\ncat(longmcq(choices))\n```\n\n3. True or false, when performing gradient descent on the model given by the equation $y = w_0 + w_1x_1 + w_2x_2$, we might arrive at a local minimum and miss the global one.\n\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\"True\", answer=\"False\")\n\ncat(longmcq(choices))\n\n```\n\n4. Which of the following would work to estimate the free parameters of a **nonlinear** model? \n\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer=\"gradient descent\",\n    \"ordinary least squares solution\",\n    \"both work\"\n)\n\ncat(longmcq(choices))\n```\n\n5. True or false, in gradient descent, we search through all possible parameters in the parameter space. \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\"True\", answer=\"False\")\n\ncat(longmcq(choices))\n\n```\n\n\n## Model fitting in R \n\nQuestions 6-9 refer to the code and output below, performing gradient descent with `optimg`: \n\n```{r}\n#| echo: false \n\nSSE <- function(data, par) {\n  data %>%\n    mutate(prediction = par[1] + par[2]* x) %>%\n    mutate(error = prediction - y) %>%\n    mutate(squared_error = error^2) %>%\n    with(sum(squared_error))\n}\n\n```\n\n```{r}\noptimg(data = data, par = c(0,0), fn=SSE, method = \"STGD\")\n```\n\n6. How many steps did the gradient descent algorithm take? `r fitb(6, width =10)`\n\n7. What was the sum of squared error of the optimal paramters?  `r fitb(959.4293, width =10)`\n\n8. What coefficients does the algorithm converge on? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"3.37930046, 0.06683237\",\n    \"0, 0\",\n    \"959.4293\", \n    \"6, 0\", \n    \"all of the above\"\n)\n\ncat(longmcq(choices))\n```\n\n9. What parameters were used to initialized the algorithm? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"3.37930046, 0.06683237\",\n    answer =\"0, 0\",\n    \"959.4293\", \n    \"6, 0\"\n)\n\ncat(longmcq(choices))\n```\n\nQuestions 10-12 refer to the output below from `lm()`: \n\n```{r}\n#| echo: false\nlm(y ~ x, data = data)\n\n```\n\n\n10. Use R notation to write the model specification. \n\n```{r}\n#| code-fold: true\n#| code-summary: \"answer\"\n#| eval: false\ny ~ x  # this works (implicit intercept)\n\ny ~ 1 + x # this also works (explicit intercept)\n\n```\n\n\n11. Given the model is specified by the equation $y = w_0+w_1x_1$, what is the parameter estimate for $w_0$ = `r fitb(3.37822, width = 10)` and $w_1$ = `r fitb(0.06688, width = 10)`. \n\n\n12. True or false, for this model, `optimg()` with gradient descent would converge on the same parameter estimates? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(answer=\"True\", \"False\")\n\ncat(longmcq(choices))\n\n```\n\n\n## Model accuracy \n\nQuestion 13 refers to the following figure: \n\n```{r}\n#| echo: false\n\nset.seed(1337)\nbeta0 = 2\nbeta1 = 0.5\ndata <- tibble(x = runif(200, min = 0, max = 10),\n               y = beta0+ (beta1*x) + rnorm(200, sd = 0.5))\n\n\n```\n\n```{r}\n#| echo: false\n\nmodel <- lm(y ~ 1 + x, data = data)\nmodel_int <- lm(y ~ 1, data = data)\n\ndata <- data %>%\n    mutate(mod_pred = predict(model)) %>%\n    mutate(mod_int = predict(model_int))\n\nggplot(\n    data, aes(x = x, y = y) \n) +\n    geom_point() +\n    geom_segment(aes(xend = x, yend = mod_pred), color = \"red\") +\n    geom_smooth(method=\"lm\", se = FALSE, formula = \"y ~ x\") \n\n```\n\n\n13. In the figure above, which of the following corresponds to the residuals? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"blue line\",\n    answer =\"red lines\",\n    \"black dots\", \n    \"none of the above\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\n14. Suppose the $R^2$ value on the model in the figure above is about 0.88. Given this value, which of the following best describes the accuracy of the model? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"fairly high accuracy \",\n    \"fairly low accuracy\",\n    \"not enough information\"\n)\n\ncat(longmcq(choices))\n\n```\n\n15. Suppose 0.88 reflects the $R^2$ for our fitted model on our sample. Which of the following is true about the $R^2$ for our fitted model on the population? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"tends to be higher\",\n    answer = \"tends to be lower\",\n    \"the same\"\n)\n\ncat(longmcq(choices))\n\n```\n\n16. Which of the following best describes an overfit model? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"performs well predicting new values, but poorly on the sample\",\n    answer = \"performs well on the sample, but poorly predicting new values\",\n    \"performs poorly both on the sample and predicting new values\",\n    \"performs well both on the sample and predicting new values\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\n17. How can we estimate $R^2$ on the population? Choose all that apply.\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer=\"cross validation\",\n    answer = \"bootstrapping\",\n    \"set.seed\", \n    \"none of the above\"\n)\n\ncat(longmcq(choices))\n\n```\n\n18. Fill in the blanks below to best describe cross validation:\n    - Leave some out\n    - Fit a model on the data `r mcq(c(\"left out\", answer =  \"kept in\"))`\n    - Evaluate the mdoel on the data `r mcq(c(answer =\"left out\",\"kept in\"))` \n\n## Model accuracy in R \n\n\nQuestions 19-20 refer to the following code and output:\n\n```{r}\nmodel <- lm(y ~ 1 + x, data)\nsummary(model)\n```\n\n19. What is the $R^2$ value for the model fit above? \n\n`r fitb(0.8822, width=10)`\n\n20. Does the value you entered in 19 reflect $R^2$ on the population or on the sample? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"population\",\n    answer = \"sample\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\nQuestions 21-23 refer to the following code and output: \n\n```{r}\n# we divide the data into v folds for cross-validation\nset.seed(2) \nsplits <- vfold_cv(data)\n\n# model secification \nmodel_spec <- \n  linear_reg() %>%  \n  set_engine(engine = \"lm\")  \n\n# add a workflow\nour_workflow <- \n  workflow() %>%\n  add_model(model_spec) %>%  \n  add_formula(y ~ x) \n\n# fit models to our folds \nfitted_models <- \n  fit_resamples(\n    object = our_workflow, \n    resamples = splits\n    ) \n\nfitted_models %>%\n    collect_metrics()\n\n```\n\n21. In the cross-validation performed above, how many folds were the data split into? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"2\",\n   \"5\",\n    answer = \"10\"\n)\n\ncat(longmcq(choices))\n\n```\n\n22. What $R^2$ do we estimate for the population? \n\n`r fitb(0.890, width = 10)`\n\n23. What model has been fit? \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"`y ~ x`\",\n    \"`y ~ x^2`\",\n   \"`x ~ y`\",\n    \"`vfold_cv`\"\n)\n\ncat(longmcq(choices))\n\n```\n\n\nQuestions 24-26 refer to the following code and output: \n\n```{r}\n# we bootstrap the data for cross-validation\nset.seed(2) \nbootstrap <- bootstraps(data) \n\n# fit models to our folds \nfitted_models_boot <- \n  fit_resamples(\n    object = our_workflow, \n    resamples = bootstrap\n    ) \n\nfitted_models_boot %>%\n    collect_metrics()\n\n```\n\n24. How many bootstrap samples did we generate? \n\n`r fitb(25, width = 10)`\n\n25. True or false, we fit the same model to the bootstrap data as we did in the cross-validation code. \n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    answer = \"TRUE\",\n    \"FALSE\"\n)\n\ncat(longmcq(choices))\n\n```\n\n26. True or false, the $R^2$ estimated by bootstrapping is equal to the $R^2$ estimated by cross-validation.\n\n```{r}\n#| echo: false\n#| results: asis\n\n# Define the answer choices\nchoices <- c(\n    \"TRUE\",\n    answer=\"FALSE\"\n)\n\ncat(longmcq(choices))\n\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css","../include/webex.css"],"toc":true,"number-sections":false,"include-in-header":["../include/webex.js"],"output-file":"lab-07.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","callout-appearance":"simple","theme":"cosmo","fontsize":"1em","linestretch":1.35,"number-depth":2,"grid":{"sidebar-width":"300px"},"title":"Practice quiz 3","subtitle":"Not graded, just practice","date":"2023-10-26","author":"Katie Schuler"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}