{
  "hash": "014e15b7b11b933568cb305527cb6ce4",
  "result": {
    "markdown": "---\ntitle: \"Sampling Distributions\"\nsubtitle: \"Data Science for Studying Language and the Mind\"\nauthor: Katie Schuler\ndate: 09-21-2023\necho: true\nformat: \n    revealjs:\n        theme: dark\n        slide-number: true\n        incremental: true \n        footer: \"[https://kathrynschuler.com/datasci](https://kathrynschuler.com/datasci/)\"\n---\n\n::: {.cell}\n\n:::\n\n\n## You are `here` {.smaller} \n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n##### Data science with R \n::: {.nonincremental}\n- Hello, world!\n- R basics\n- Data importing\n- Data visualization\n- Data wrangling \n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### Stats & Model buidling\n::: {.nonincremental}\n- `Probability distributions`\n- `Sampling variability`\n- Hypothesis testing\n- Model specification\n- Model fitting \n- Model accuracy\n- Model reliability\n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### More advanced \n::: {.nonincremental}\n\n- Classification\n- Feature engineering (preprocessing) \n- Inference for regression\n- Mixed-effect models\n::: \n:::\n\n::::\n\n## Attribution\n\n- Inspired by a MATLAB course Katie took by Kendrick Kay \n- Data simulated from Ritchie et al 2018:\n\n. . . \n\n> Sex Differences in the Adult Human Brain: Evidence from 5216 UK Biobank Participants\n\n# Explore a simple dataset \n\n## Dataset {.smaller}\n\nSuppose we measure a single quantity: `brain volume of human adults` \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nRows: 5,216\nColumns: 1\n$ volume <dbl> 1193.283, 1150.383, 1242.702, 1206.808, 1235.955, 1292.399, 120…\n```\n:::\n\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n\n## Visualize the distribution {.smaller}\n\nVisualize the distribution of the data with a `histogram`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n\n\n## Measure of central tendency {.smaller}\n\nSummarize the data with a single value: `mean`, a measure of where a central or typical value might fall\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nsum_stats <- data %>% summarise(\n    n = n(), \n    mean = mean(volume))\nsum_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n      n  mean\n  <int> <dbl>\n1  5216 1173.\n```\n:::\n:::\n\n\n. . . \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n\n## Measure of variability {.smaller}\n\nSummarize the spread of the data with `standard deviation`\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nsum_stats <- data %>% summarise(\n    n = n(), \n    mean = mean(volume),\n    sd = sd(volume))\nsum_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n      n  mean    sd\n  <int> <dbl> <dbl>\n1  5216 1173.  112.\n```\n:::\n:::\n\n\n. . . \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n\n\n\n## Parametric statistics\n\nMean and sd are `parametric` summary statistics. They are given by the following equations:\n\n:::: {.columns}\n::: {.column width=50%}\n$mean(x) = \\bar{x} = \\frac{\\sum_{i=i}^{n} x_{i}}{n}$\n:::\n::: {.column width=50%}\nsd($x$) = $\\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}}$\n:::\n::::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n\n\n- Mean and sd are a good summary of the data when the distribution is `normal` (**gaussian**)\n\n## Nonparametric statistics\n\nBut suppose our distribution is not normal.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n\n## Nonparametric statistics\n\nmean and sd are not a good summary anymore.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## Median {.smaller}\n\nInstead we can use the median as our measure of central tendency.\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nnp_sum_stats <- not_normal %>% summarise(\n    n = n(), \n    median = median(y))\nnp_sum_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n      n median\n  <int>  <dbl>\n1   111     15\n```\n:::\n:::\n\n\n. . . \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n## IQR {.smaller}\n\nAnd the interquartile range (`IQR`) as a measure of the spread in our data.\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nnp_sum_stats <- not_normal %>% summarise(\n    n = n(), \n    median = median(y),\n    lower = quantile(y, 0.25),\n    upper = quantile(y, 0.75) )\nnp_sum_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n      n median lower upper\n  <int>  <dbl> <dbl> <dbl>\n1   111     15     5    25\n```\n:::\n:::\n\n\n. . . \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n\n# Probability distributions  {.smaller}\n\nA mathematical function that describes the probability of observing different possible values of a variable\n\n## Uniform probability distribution  {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-15-1.png){width=960}\n:::\n:::\n\n\n## Uniform probability distirubtion {.smaller}\n\nAll possible values are equally likely\n\n:::: {.columns}\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=50%}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuniform_sample %>% summarise(\n    min = min(y), \n    max = max(y), \n    prob = 1/(max - min))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n    min   max  prob\n  <int> <int> <dbl>\n1     1    10 0.111\n```\n:::\n:::\n\n\n\n\nheight of prob density func\n\n::: {.cell}\n\n```{.r .cell-code}\ndunif(4, min = 1, max = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1111111\n```\n:::\n:::\n\n\n\nprob less than given value\n\n::: {.cell}\n\n```{.r .cell-code}\npunif(4, min = 1, max = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3333333\n```\n:::\n:::\n\n\n:::\n::::\n\n$p(x) = \\frac{1}{max-min}$\n\n\n\n## Gaussian (normal) probability distribution {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n\n## Gaussian (normal) probability distribution {.smaller}\n\n:::: {.columns}\n::: {.column width=50%}\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=50%}\n\n\nheight of prob density func\n\n::: {.cell}\n\n```{.r .cell-code}\n#dunif(4, min = 1, max = 10)\ndnorm(4, mean=0, sd=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0001338302\n```\n:::\n:::\n\n\n\nprob less than given value\n\n::: {.cell}\n\n```{.r .cell-code}\n#punif(4, min = 1, max = 10)\npnorm(4, mean=0, sd=1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9999683\n```\n:::\n:::\n\n\n:::\n::::\n\n$p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)$\n\n# Sampling variability\n\n## The population {.smaller}\n\nWe actually want to know something about the `population`: the mean brain volume of Penn undergrads (the **parameter**)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n\n## The sample  {.smaller}\n\nBut we only have a small `sample` of the  population: maybe we can measure the brain volume of 100 students\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n\n## Sampling variability {.smaller}\n\nAny statistic we compute from a random sample we've collected (**parameter estimate**) will be subject to `sampling variability` and will differ from that statistics computed on the entire population (**parameter**)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n\n\n## Sampling variability {.smaller}\n\nIf we took another sample of 100 students, our parameter estimate would be different. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n\n## Sampling distribution {.smaller}\n\nThe `sampling distribution` is the probability distribution of values our parameter estimate can take on. Constructed by taking a random sample, computing stat of interest, and repeating many times.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n\n## Quantifying sampling variability {.smaller}\n\nThe `spread` of the sampling distribution indicates how the parameter estimate will vary from different random samples. We can quantify the spread (express our uncertainty on our parameter estimate) in two ways\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-29-1.png){width=960}\n:::\n:::\n\n\n## Quantifying sampling variability with `standard error` {.smaller}\n\nOne way is to compute the standard deviation of the sampling distribution: the `standard error`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-30-1.png){width=960}\n:::\n:::\n\n\n## Quantifying sampling variability with a `confidence interval`{.smaller}\n\nAnother way is to construct a `confidence interval`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-31-1.png){width=960}\n:::\n:::\n\n\n## Practical considerations \n\n- We don't have access to the entire population\n- We can (usually) only do our experiment once\n\n\n\n\n\n\n\n# Bootstrapping\n\nTo construct the sampling distribution\n\n## Bootstrapping \n\nInstead of assuming a parametric probability distributon, we use the data themselves to approximate the underlying distribution: we `sample our sample`!\n\n\n# Bootsrapping with `infer` \n## `infer` is part of `tidymodels` \n\n> The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\n```r\ninstall.packages(\"tidymodels\")`\n```\n\n\n::: {.cell}\n\n:::\n\n\n## Generate the sampling distribution {.smaller}\n\nGenerate the sampling distribution with `specify()`, `generate()`, and `calculate()`\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nbootstrap_distribution <- sample1  %>% \n  specify(response = volume) %>% \n  generate(reps = 1000, type = \"bootstrap\") %>% \n  calculate(stat = \"mean\")\n\nbootstrap_distribution\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: volume (numeric)\n# A tibble: 1,000 × 2\n   replicate  stat\n       <int> <dbl>\n 1         1 1238.\n 2         2 1262.\n 3         3 1259.\n 4         4 1232.\n 5         5 1226.\n 6         6 1294.\n 7         7 1259.\n 8         8 1246.\n 9         9 1226.\n10        10 1235.\n# ℹ 990 more rows\n```\n:::\n:::\n\n\n## Visualize the bootstrap distribution {.smaller}\n\nVisualize the bootstrap distribution you generated with `visualize()`\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nbootstrap_distribution %>% \n  visualize()\n```\n\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n\n## Quantify the spread with `se` {.smaller}\n\nQuantify the spread of the sampling distributon with `get_confidence_interval()`, using **standard error**\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nse_bootstrap <- bootstrap_distribution %>% \n  get_confidence_interval(\n    type = \"se\",\n    point_estimate = mean(sample1$volume)\n  )\n\nse_bootstrap\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    1210.    1274.\n```\n:::\n:::\n\n\n. . . \n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nbootstrap_distribution %>% \n  visualize() +\n  shade_confidence_interval(\n    endpoints = se_bootstrap\n  )\n```\n\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-36-1.png){width=960}\n:::\n:::\n\n\n## Quantify the spread with `ci` {.smaller}\n\nQuantify the spread of the sampling distributon with `get_confidence_interval`, using a **confidence interval**\n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nci_bootstrap <- bootstrap_distribution %>% \n  get_confidence_interval(\n    type  =\"percentile\", \n    level = 0.95\n  )\n\nci_bootstrap\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     <dbl>    <dbl>\n1    1211.    1273.\n```\n:::\n:::\n\n\n. . . \n\n\n::: {.cell output-location='column'}\n\n```{.r .cell-code}\nbootstrap_distribution %>% \n  visualize() +\n  shade_confidence_interval(\n    endpoints = ci_bootstrap\n  )\n```\n\n::: {.cell-output-display}\n![](sampling-distribution_files/figure-revealjs/unnamed-chunk-38-1.png){width=960}\n:::\n:::",
    "supporting": [
      "sampling-distribution_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}