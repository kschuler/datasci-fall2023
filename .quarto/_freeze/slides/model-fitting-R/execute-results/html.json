{
  "hash": "bd13d8efa9a6a9a1891b6b5e378b0837",
  "result": {
    "markdown": "---\ntitle: \"Model Fitting with R (demo)\"\nsubtitle: \"Data Science for Studying Language and the Mind\"\nauthor: Katie Schuler\ndate: 10-17-2023\necho: true\nformat: \n    revealjs:\n        theme: dark\n        slide-number: true\n        incremental: true \n        footer: \"[https://kathrynschuler.com/datasci](https://kathrynschuler.com/datasci/)\"\n---\n\n::: {.cell}\n\n:::\n\n\n## You are `here` {.smaller} \n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n##### Data science with R \n::: {.nonincremental}\n- Hello, world!\n- R basics\n- Data importing\n- Data visualization\n- Data wrangling \n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### Stats & Model buidling\n::: {.nonincremental}\n- Sampling distribution\n- Hypothesis testing\n- Model specification\n- `Model fitting` \n- Model accuracy\n- Model reliability\n:::\n:::\n\n::: {.column width=\"33%\"}\n\n##### More advanced \n::: {.nonincremental}\n\n- Classification\n- Feature engineering (preprocessing) \n- Inference for regression\n- Mixed-effect models\n::: \n:::\n\n::::\n\n## Roadmap {.smaller}\n\n- Fit a model in R \n- Goodness of fit: quantifying our intuition \n- How do we estimate the free parameters? \n    1. Gradient descent - iterative optimization algorithm\n    2. Ordinary least squares - analytical solution for linear regression\n- If time: another full example\n\n## Fit a model  {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n| | model specification | \n| -- | -- |\n| R syntax | `rt ~ 1 + experience` |\n| R syntax | `rt ~ experience` |\n| Equation | $y=w_0+w_1x_1$ |\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n```{mermaid}\n%%| echo: false\nflowchart TD\n    spec(Model specification) --> fit(Estimate free parameters) \n    fit(Estimate free parameters) --> fitted(Fitted model) \n```\n\n\n$y = 211.271 + -1.695x$\n:::\n::::\n\n## Fit a model in R {.smaller}\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n\n| | model specification| \n| -- | -- |\n| R syntax | `rt ~ 1 + experience` |\n| R syntax | `rt ~ experience` |\n| Equation | $y=w_0+w_1x_1$ |\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(rt ~ experience, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rt ~ experience, data = data)\n\nCoefficients:\n(Intercept)   experience  \n    211.271       -1.695  \n```\n:::\n:::\n\n\n:::\n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>% \n    specify(rt ~ experience) %>%\n    fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  term       estimate\n  <chr>         <dbl>\n1 intercept    211.  \n2 experience    -1.69\n```\n:::\n:::\n\n\n::: \n\n::: {.fragment}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() %>%\n    set_engine(\"lm\") %>%\n    fit(rt ~ experience, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = rt ~ experience, data = data)\n\nCoefficients:\n(Intercept)   experience  \n    211.271       -1.695  \n```\n:::\n:::\n\n\n\n:::\n:::\n::::\n\n\n\n## Goodness of fit {.smaller}\n\nQuantifying our intution with sum of squared error\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 16px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> experience </th>\n   <th style=\"text-align:right;\"> rt </th>\n   <th style=\"text-align:right;\"> prediction </th>\n   <th style=\"text-align:right;\"> error </th>\n   <th style=\"text-align:right;\"> squared_error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 49 </td>\n   <td style=\"text-align:right;\"> 124 </td>\n   <td style=\"text-align:right;\"> 128.216 </td>\n   <td style=\"text-align:right;\"> 4.216 </td>\n   <td style=\"text-align:right;\"> 17.774656 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 69 </td>\n   <td style=\"text-align:right;\"> 95 </td>\n   <td style=\"text-align:right;\"> 94.316 </td>\n   <td style=\"text-align:right;\"> -0.684 </td>\n   <td style=\"text-align:right;\"> 0.467856 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 89 </td>\n   <td style=\"text-align:right;\"> 71 </td>\n   <td style=\"text-align:right;\"> 60.416 </td>\n   <td style=\"text-align:right;\"> -10.584 </td>\n   <td style=\"text-align:right;\"> 112.021056 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 99 </td>\n   <td style=\"text-align:right;\"> 45 </td>\n   <td style=\"text-align:right;\"> 43.466 </td>\n   <td style=\"text-align:right;\"> -1.534 </td>\n   <td style=\"text-align:right;\"> 2.353156 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 109 </td>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:right;\"> 26.516 </td>\n   <td style=\"text-align:right;\"> 8.516 </td>\n   <td style=\"text-align:right;\"> 72.522256 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n. . . \n\n$SSE=\\sum_{i=i}^{n} (d_{i} - m_{i})^2 = 205.139$ \n\n\n## Sum of squared error {.smaller}\n\nGiven some data: \n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 18px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> experience </th>\n   <th style=\"text-align:right;\"> rt </th>\n   <th style=\"text-align:right;\"> prediction </th>\n   <th style=\"text-align:right;\"> error </th>\n   <th style=\"text-align:right;\"> squared_error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 49 </td>\n   <td style=\"text-align:right;\"> 124 </td>\n   <td style=\"text-align:right;\"> 128.216 </td>\n   <td style=\"text-align:right;\"> 4.216 </td>\n   <td style=\"text-align:right;\"> 17.774656 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 69 </td>\n   <td style=\"text-align:right;\"> 95 </td>\n   <td style=\"text-align:right;\"> 94.316 </td>\n   <td style=\"text-align:right;\"> -0.684 </td>\n   <td style=\"text-align:right;\"> 0.467856 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 89 </td>\n   <td style=\"text-align:right;\"> 71 </td>\n   <td style=\"text-align:right;\"> 60.416 </td>\n   <td style=\"text-align:right;\"> -10.584 </td>\n   <td style=\"text-align:right;\"> 112.021056 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 99 </td>\n   <td style=\"text-align:right;\"> 45 </td>\n   <td style=\"text-align:right;\"> 43.466 </td>\n   <td style=\"text-align:right;\"> -1.534 </td>\n   <td style=\"text-align:right;\"> 2.353156 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 109 </td>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:right;\"> 26.516 </td>\n   <td style=\"text-align:right;\"> 8.516 </td>\n   <td style=\"text-align:right;\"> 72.522256 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nCompute the sum of squared error: \n\n$SSE=\\sum_{i=i}^{n} (d_{i} - m_{i})^2 = 205.139$ \n\n. . . \n\n\n::: {.cell}\n\n```{.r .cell-code}\n  data %>%\n    mutate(prediction = 211.271 + -1.695 * experience) %>%\n    mutate(error = prediction - rt) %>%\n    mutate(squared_error = error^2) %>%\n    with(sum(squared_error))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 205.139\n```\n:::\n:::\n\n\n## Gradient descent \n\nA search problem: we have a parameter space, a cost function, and our job is to search through the space to find the point that minimizes the cost function.\n\n\n![](../include/figures/gradient-descent.png)\n\n\n## Gradient descent {.smaller}\n\nDefine our cost function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSSE <- function(data, par) {\n  data %>%\n    mutate(prediction = par[1] + par[2]* experience) %>%\n    mutate(error = prediction - rt) %>%\n    mutate(squared_error = error^2) %>%\n    with(sum(squared_error))\n}\n\nSSE(data = data, par = c(0, 0))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 31791\n```\n:::\n:::\n\n\n. . . \n\nImpliment gradient descent algorithm with `optimg`\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptimg(\n    data = data,  # our data\n    par = c(0,0), # our parameters\n    fn = SSE,     # our cost function\n    method = \"STGD\") # our iterative optimization algorithm \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$par\n[1] 211.26155  -1.69473\n\n$value\n[1] 205.138\n\n$counts\n[1] 12\n\n$convergence\n[1] 0\n```\n:::\n:::\n\n\n## Ordinary least squares solution {.smaller}\n\n$y = w_0 + w_1x_1$\n\n. . . \n\nWe have a system of equations: \n\n- $124 = w_01 + w_149$\n- $95 = w_01 + w_169$ \n- ...\n- $18 = w_01 + w_1109$\n\n. . . \n\nWe can express them as a matrix: \n$Y = Xw + \\epsilon$ \n\n. . . \n\nAnd solve with linear algebra:\n$w = (X^TX)^{-1}X^TY$\n\n<!-- \\[\n\\begin{bmatrix}\n124\\\\\n95\\\\\n71\\\\\n45\\\\\n18\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 49\\\\\n1 & 69\\\\\n1 & 79\\\\\n1 & 89\\\\\n1 & 99\\\\\n1 & 109\\\\\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\nw_0\\\\\nw_1\\\\\n\\end{bmatrix} \n\\] -->\n\n## Ordinary least squares solution in R {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n ols_matrix_way <- function(X, Y){\n  solve(t(X) %*% X) %*% t(X) %*% Y\n }\n```\n:::\n\n\n. . . \n\nWe need to construct X and Y (must be matrices): \n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n(response_matrix <- data %>% select(rt) %>% as.matrix())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      rt\n[1,] 124\n[2,]  95\n[3,]  71\n[4,]  45\n[5,]  18\n```\n:::\n\n```{.r .cell-code}\n(explanatory_matrix <- data %>% mutate(int = 1) %>% select(int, experience) %>% as.matrix())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     int experience\n[1,]   1         49\n[2,]   1         69\n[3,]   1         89\n[4,]   1         99\n[5,]   1        109\n```\n:::\n:::\n\n\n. . . \n\nThen we can use our function to generate the OLS solution: \n\n::: {.cell}\n\n```{.r .cell-code}\nols_matrix_way(explanatory_matrix, response_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   rt\nint        211.270690\nexperience  -1.694828\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(rt ~ experience, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = rt ~ experience, data = data)\n\nCoefficients:\n(Intercept)   experience  \n    211.271       -1.695  \n```\n:::\n:::\n\n<!-- \n\n## Linear model functional form\n\n| field | linear model eq |\n| --- | --------- |\n| `h.s. algebra` | $y=ax+b$ |\n| `machine learning` | $y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$ |\n| `statistics` | $y = β_0 + β_1x_1 + β_2x_2 + ... + β_nx_n + ε$ |\n| `matrix` | $y = Xβ + ε$ |\n\n\n## Fitting a linear model {.smaller}\n\n:::: {.columns}\n\n::: {.column width=\"33%\"}\n\n\n\n\n\n:::\n\n::: {.column width=\"67%\"}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data, aes(x = x, y = y)) +\n    geom_point(size = 4, color = \"darkred\") +\n    geom_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE) \n```\n\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-18-1.png){width=960}\n:::\n:::\n\n\n:::\n\n\n\n::::\n\n## Fitting by intuition {.smaller}\n\nHow would you draw a \"best fit\" line?  \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n## Fitting by intuition {.smaller}\n\nWhich line fits best?  How can you tell? \n\n\n::: {.cell layout-nrow=\"3\" layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-20-2.png){width=960}\n:::\n:::\n\n\n\n## Quantifying \"goodness\" of fit  {.smaller}\n\nWe can measure how close the model is to the data\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-22-2.png){width=960}\n:::\n:::\n\n\n. . . \n\n`residuals`\n\n## {.smaller} \n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](model-fitting-R_files/figure-revealjs/unnamed-chunk-23-2.png){width=960}\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> x </th>\n   <th style=\"text-align:right;\"> y </th>\n   <th style=\"text-align:right;\"> pred </th>\n   <th style=\"text-align:right;\"> err </th>\n   <th style=\"text-align:right;\"> sq_err </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1.2 </td>\n   <td style=\"text-align:right;\"> 1.3 </td>\n   <td style=\"text-align:right;\"> -0.1 </td>\n   <td style=\"text-align:right;\"> 0.01 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2.5 </td>\n   <td style=\"text-align:right;\"> 2.0 </td>\n   <td style=\"text-align:right;\"> 0.5 </td>\n   <td style=\"text-align:right;\"> 0.25 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2.3 </td>\n   <td style=\"text-align:right;\"> 2.7 </td>\n   <td style=\"text-align:right;\"> -0.4 </td>\n   <td style=\"text-align:right;\"> 0.16 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 3.1 </td>\n   <td style=\"text-align:right;\"> 3.4 </td>\n   <td style=\"text-align:right;\"> -0.3 </td>\n   <td style=\"text-align:right;\"> 0.09 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 4.4 </td>\n   <td style=\"text-align:right;\"> 4.1 </td>\n   <td style=\"text-align:right;\"> 0.3 </td>\n   <td style=\"text-align:right;\"> 0.09 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> x </th>\n   <th style=\"text-align:right;\"> y </th>\n   <th style=\"text-align:right;\"> pred </th>\n   <th style=\"text-align:right;\"> err </th>\n   <th style=\"text-align:right;\"> sq_err </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1.2 </td>\n   <td style=\"text-align:right;\"> 1.58 </td>\n   <td style=\"text-align:right;\"> -0.38 </td>\n   <td style=\"text-align:right;\"> 0.1444 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 2.5 </td>\n   <td style=\"text-align:right;\"> 2.62 </td>\n   <td style=\"text-align:right;\"> -0.12 </td>\n   <td style=\"text-align:right;\"> 0.0144 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 3 </td>\n   <td style=\"text-align:right;\"> 2.3 </td>\n   <td style=\"text-align:right;\"> 3.66 </td>\n   <td style=\"text-align:right;\"> -1.36 </td>\n   <td style=\"text-align:right;\"> 1.8496 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 4 </td>\n   <td style=\"text-align:right;\"> 3.1 </td>\n   <td style=\"text-align:right;\"> 4.70 </td>\n   <td style=\"text-align:right;\"> -1.60 </td>\n   <td style=\"text-align:right;\"> 2.5600 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 5 </td>\n   <td style=\"text-align:right;\"> 4.4 </td>\n   <td style=\"text-align:right;\"> 5.74 </td>\n   <td style=\"text-align:right;\"> -1.34 </td>\n   <td style=\"text-align:right;\"> 1.7956 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## But there are infinite possibilities\n\nWe can't test all `Inf` of the possible free parameters\n\n$y=b_0+b_1x_1$\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### Free parameters to test\n\n![](../include/figures/error-surf-1.png)\n\n:::\n\n::: {.column width=\"50%\"}\n\n#### Level = SSE\n\n![](../include/figures/error-surf-2.png)\n\n:::\n\n::::\n\n\n## Error surface \n\n![](../include/figures/error-surface.png)\n\n## Gradient descent, intuition\n\n![](../include/figures/grad-desc-intuition.jpeg)\n\n## Gradient descent \n\n![](../include/figures/gradient-descent.png)\n\n## Gradient descent linear model \n\n![](../include/figures/error-surface-linear.png)\n\nLinear models are convex functions: one minimum\n\n## Ordinary least squares\n\nLinear models have a solution: we can solve for the values with linear algebra. \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### $y = ax + b$\n\n$1.2 = a1 + b$\n\n$2.5 = a2 + b$\n\n::: \n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ 1 + x, data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ 1 + x, data = data)\n\nCoefficients:\n(Intercept)            x  \n        0.6          0.7  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n    specify(y ~ 1 + x) %>%\n    fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept    0.600\n2 x            0.7  \n```\n:::\n:::\n\n\n:::\n\n::::\n\n`ordinary least squares`\n\n\n\n\n\n\n\n\n\n\n\n -->\n",
    "supporting": [
      "model-fitting-R_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}