{
  "hash": "a283dd4ac4aea76526c4128e016e3c84",
  "result": {
    "markdown": "---\ntitle:  \"Probablity distributions\"\ndate:   2023-09-12\nauthor: \n    - Katie Schuler\nexecute:\n  echo: true\n---\n\n\n::: {.callout-warning title=\"Under Construction\"}\n:::\n\n## Exploring a simple dataset\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Setup code\"}\n# ------ setup for today's lecture notes ----- # \n\n# suppress startup messages at package load\nsuppressPackageStartupMessages(library(tidyverse))\n\n# set the theme for the plots \ntheme_set(theme_classic(base_size=15))\n\n# generate 10000 y values with mean 3 and sd 0.25\ndata <- tibble(\n    y=rnorm(1000, mean=3, sd=0.25)\n) \n```\n:::\n\n\n\nWe begin with the simplest possible dataset: suppose we measure a single quantity `y`. What can we do with these data? \n\nWe can create a visual summary of our dataset with a **histogram**. A histogram plots the distribution of a set of data, which allows us to get a quick visual of the data: formally we have plotted the the frequency distribution (count) of the data, but this also gives a sense of the central tendency and variability in our dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data=data, aes(x=y)) +\n    geom_histogram(\n        binwidth = 0.25,\n        color=\"black\", fill='lightgray', alpha=0.5\n    ) \n```\n\n::: {.cell-output-display}\n![](probability-distributions_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Descriptive statistics\nWe can summarize (or describe) a set of data with **descriptive statistics**. There are three types of measures:\n\n- **central tendency** describes a central or typical value (mean, median, mode)\n- **variability** describes dispersion or spread of values (variance, standard deviation, range, IQR)\n- **frequency distribution** describes how frequently different values occur (count)\n\nR has built-in functions to handle descriptive statistics (we saw these in lecture 1): \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata %>%\n    summarise(\n        n = n(), \n        mean = mean(y),\n        median = median(y),\n        sd = sd(y),\n        iqr_lower = quantile(y, 0.25),\n        iqr_upper = quantile(y, 0.75)\n    ) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 6\n      n  mean median    sd iqr_lower iqr_upper\n  <int> <dbl>  <dbl> <dbl>     <dbl>     <dbl>\n1  1000  3.00   3.00 0.251      2.83      3.16\n```\n:::\n:::\n\n\n## Parametric vs. nonparametric \nSome statistics are considered **paramteric** because they make assumptions about the the distribution of the data (can therefore be computed theoretically from parameters):  \n\n- The mean and standard deviation assume the distribution is Gaussian and can therefore be computed via the following equations \n- **mean** $\\mu$ \n- **sd** $\\sigma$\n\nOther statstics are **nonparametric** because they make minimal assumptions about the distribution of the data: \n\n- **median** is the 50th percentile, the value below which 50% of the data points fall. \n- **inter-quartile range (IQR)** is the difference between the 25th and 75th percentiles (sometimes called the **50% coverage interval** because 50% of the data fall in this range).  \n- Note that we can calculate any arbitrary coverage interval. The 95% coverage interval — widely used in the sciences — is the difference between the 2.5 percentile and the 97.5 percentile, including all but 5% of the data. \n\n\n## Probability distributions\n\nA **probability distribution** (aka probability density function) is a mathematical function that describes the probability of observing the different possible values of a variable (or variables). We will focus on univariate distributions in this class — probability distributions of just one random variable — but probability distributions can also be multivariate. \n\n- One of the simplest probability distributions is the **uniform distribution**, where all possible values of a variable are equally likely. The probability density function for the uniform distribution is given by the following equation with two parameters (the boundaries, min and max): \n  - $p(x) = \\frac{1}{max-min}$\n- One of the most useful probability distributions for our purposes is the **Gaussian (or Normal) distribution**. The probability density function for the Gaussian distribution is given by the following equation, with the parameters $\\mu$ (mean) and $\\sigma$ (standard deviation):\n  - $p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)$\n  - The Gaussian distribution assumes that the distribution of a set of data takes a certain form (is unimodal, symmetric, etc).\n  - When values are sampled from a Gaussian distribution, 68% of the values will be within one standard deviation from the mean and 95% within two standard deviations from the mean.\n  - When computing the mean and standard deviation of a set of data, we are fitting a Gaussian distribution to the data. \n\n## Probability distributions with R \n\nThe probability distributions we've discussed so far are considered \"parametric\" because they are given by one or more parameters. When we use R's functions to generate values from these distributions, we provide these parameters as arguments. Base R has four functions we will use to generate values associated with a probability distribution. \n- `dnorm(mean=5, sd=1)` returns the height of the **probability density function** at the given values\n- `pnorm(5, mean=5, sd=1)` returns the **cumulative density function** (the probability that a random number from the distribution will be less than the given values)\n- `qnorm(0.8, mean=5, sd=1)` returns the value whose cumulative distribution matches the probability (**inverse of `p`**)\n- `rnorm(1000, mean=5, sd=1)` returns n **random numbers** generated from the distribution\n\nTo use another distribution, change the function's suffix to the name of the distribution and the parameters to those that define the distribution. For example, to generate n random numbers from a uniform distribution with a min of 1 and a max of 5, run `runif(n, min=0, max=1)`. \n\n## Nonparametric probability distributions\n\nWhat if the data does not meet the assumptions of the Gaussian distribution? One option is to choose another parametric probability distribution (run `help(Distributions)` for a full list of available distributions). Another is to use a nonparametric approach, where the probability distribution is not determined by parameters but is instead determined by the data.\n- A **histogram** is actually a simple, nonparametric estimate of a probability distribution. To estimate the probability distribution that generated a set of data from a histogram, we modify the scale of the y-axis so that the total area of the bars is equal to 1. \n- **Kernel density estimation (KDE)** is another nonparametric method to estimate a probability distribution. KDE is like a smooth histogram, accomplished by placeing a kernel — a tiny Gaussian distribution — at each observed data point and summing across kernels. We can accomplish this in ggplot with the `geom_density()` geom.\n\n## Further reading and references\n\nSuggested further reading:\n\n- [Basics of descriptive statistics](https://dlf.uzh.ch/openbooks/statisticsforlinguists/chapter/basics-of-descriptive-statistics/) in Statistics for linguists\n- [Appendix A: Statistical Background](https://moderndive.com/A-appendixA.html) in Modern Dive\n- [Ch 11: Modeling Randomness](https://dtkaplan.github.io/SM2-bookdown/modeling-randomness.html) in Statistical Modeling\n\nhttps://cran.r-project.org/web/packages/infer/vignettes/infer.html\n\nOther references: \n\nhttps://r4ds.hadley.nz/data-visualize#visualizing-distributions\n\n",
    "supporting": [
      "probability-distributions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}