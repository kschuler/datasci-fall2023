{
  "hash": "e354a426d08cdae3c022610e732a489d",
  "result": {
    "markdown": "---\ntitle: \"Model fitting\"\nauthor: \n    - Katie Schuler\ndate: 2023-10-05\n---\n\n\n::: {.callout-warning title=\"Under Construction\"}\n:::\n\n\n::: {.cell}\n\n:::\n\n\n## You are here\n\n## Model building \n\n## Linear model review\n\n- how would we specify this model \n- how would we write it in R \n\n- estimate the free paramters \n\n## Estimate free parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# with base R \nlm(y ~ 1 + x, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ 1 + x, data = data)\n\nCoefficients:\n(Intercept)            x  \n        0.6          0.7  \n```\n:::\n\n```{.r .cell-code}\n# with infer workflow \ndata %>%\n    specify(y ~ 1 + x) %>%\n    fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept    0.600\n2 x            0.7  \n```\n:::\n:::\n\n\n\n\n## Model fitting basics\n\n\nLinear model \n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 2\n      x     y\n  <dbl> <dbl>\n1     1   1.2\n2     2   2.5\n3     3   2.3\n4     4   3.1\n5     5   4.4\n```\n:::\n\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can see which fits better with our eyes. \n\n\n::: {.cell layout-nrow=\"3\" layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n::: {.cell layout-nrow=\"3\" layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n|  x|   y| pred|  err| sq_err|\n|--:|---:|----:|----:|------:|\n|  1| 1.2|  1.3| -0.1|   0.01|\n|  2| 2.5|  2.0|  0.5|   0.25|\n|  3| 2.3|  2.7| -0.4|   0.16|\n|  4| 3.1|  3.4| -0.3|   0.09|\n|  5| 4.4|  4.1|  0.3|   0.09|\n:::\n\n::: {.cell-output-display}\n|  x|   y| pred|   err| sq_err|\n|--:|---:|----:|-----:|------:|\n|  1| 1.2| 1.58| -0.38| 0.1444|\n|  2| 2.5| 2.62| -0.12| 0.0144|\n|  3| 2.3| 3.66| -1.36| 1.8496|\n|  4| 3.1| 4.70| -1.60| 2.5600|\n|  5| 4.4| 5.74| -1.34| 1.7956|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(mseA$sq_err)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.12\n```\n:::\n\n```{.r .cell-code}\nmean(mseB$sq_err)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.2728\n```\n:::\n:::\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-7-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(y ~ 1 + x, data = data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ 1 + x, data = data)\n\nCoefficients:\n(Intercept)            x  \n        0.6          0.7  \n```\n:::\n\n```{.r .cell-code}\ndata %>%\n    specify(y ~ 1 + x) %>%\n    fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 2\n  term      estimate\n  <chr>        <dbl>\n1 intercept    0.600\n2 x            0.7  \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- seq(from = 0, to = 3, by = 0.1)\nb1 <- seq(from = 0, to = 3, by = 0.1)\npossible_weights <- expand.grid(b0 = b0, b1 = b1)\n\nggplot(data = possible_weights, \n    mapping = aes(x = b0, y = b1)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# compute the sum of squares for those weights on a dataframe\nsum_squares <- function(b0, b1) {\n\n    data %>% \n        mutate(pred = b0 + b1*x) %>%\n        mutate(err = pred-y) %>%\n        mutate(sq_err = err^2) %>%\n        select(sq_err) %>%\n        sum()\n   \n}\nerror_surf <- possible_weights %>% \n    rowwise() %>%\n    mutate(sum_sq =  sum_squares(b0, b1)) %>%\n    ungroup\n\nerror_surf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 961 × 3\n      b0    b1 sum_sq\n   <dbl> <dbl>  <dbl>\n 1   0       0   42.0\n 2   0.1     0   39.3\n 3   0.2     0   36.8\n 4   0.3     0   34.3\n 5   0.4     0   32.0\n 6   0.5     0   29.7\n 7   0.6     0   27.6\n 8   0.7     0   25.5\n 9   0.8     0   23.6\n10   0.9     0   21.7\n# ℹ 951 more rows\n```\n:::\n\n```{.r .cell-code}\nerror_surf  %>% filter(sum_sq < 0.608)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n     b0    b1 sum_sq\n  <dbl> <dbl>  <dbl>\n1   0.6   0.7  0.600\n```\n:::\n\n```{.r .cell-code}\nggplot(error_surf, aes(b0, b1, z = sum_sq)) +\n    geom_contour_filled()\n```\n\n::: {.cell-output-display}\n![](model-fitting_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n```{.r .cell-code}\n        # %>%\n        # sum(.$sq_err)\n       # summarise(sum_sq = sum(sq_err)) %>%\n\n\n# return sum of squares as a column next to \n\n# mse <- function(data, b0, b1) {\n#   model_value <- b0 + b1*data[1]\n#   resid <- data[2] - model_value\n#   sq_err <- resid^2\n#   sum(sq_err)\n# }\n\n# possible_weights %>% mutate(\n#     mse = mse(1, 1, b0, b1)\n# )\n```\n:::\n\n\n- in context of model building more broadly \n- a genear overview of the concept \n\n## Mean squared error \n\nCost function. \n\n## Error surface\n\n- We can visualize the error surface for simple example: 2 parameters, $\\beta_0$ and $\\beta_1$, and the cost function (mean square error). \n- Show nonlinear model v linear model figs \n- goal is to find the minimum point\n- notice the nonlinear model can have local minimums but lm has only 1. Because lm is a **convex** function. \n\n## Gradient descent \n\nIF we want to estimate the free parameters in a way that would work broadly, for linear or nonlinear models, we can use **gradient descent**. \n\n- machine learning / optimization. \n- If we have a lot of data, we could use **stochastic gradient descent** which is the same except we... \n\n## Ordinary least squares \n\nAs we saw above, linear models have the special property that they have a solution, the OLS. Rather than searching the error surface iteratively via gradient descent (optimization), we can solve for this point directly with **linear algebra**. \n\n- matrix approach: we write the 3-step function. \n- use lm() in R. \n- infer approach: \n    - specify(), fit() \n\n\n\n### Further reading \n\n- [Ch. 8 Fitting models to data](https://dtkaplan.github.io/SM2-bookdown/fitting-models-to-data.html) in Statistical Modeling\n\n\n\n",
    "supporting": [
      "model-fitting_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}