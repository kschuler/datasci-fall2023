---
title: "Model Fitting"
subtitle: "Data Science for Studying Language and the Mind"
author: Katie Schuler
date: 10-17-2023
echo: true
format: 
    revealjs:
        theme: dark
        slide-number: true
        incremental: true 
        footer: "[https://kathrynschuler.com/datasci](https://kathrynschuler.com/datasci/)"
---

```{r}
#| echo: false
#| message: false
library(tidyverse)
library(modelr)
library(infer)
library(knitr)
theme_set(theme_classic(base_size = 20))

# setup data 
data <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(1.2, 2.5, 2.3, 3.1, 4.4)
)

data2 <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(2, 2.5, 3.3, 4.1, 6.4)


)

```

## You are `here` {.smaller} 

:::: {.columns}

::: {.column width="33%"}

##### Data science with R 
::: {.nonincremental}
- Hello, world!
- R basics
- Data importing
- Data visualization
- Data wrangling 
:::
:::

::: {.column width="33%"}

##### Stats & Model buidling
::: {.nonincremental}
- Sampling distribution
- Hypothesis testing
- Model specification
- `Model fitting` 
- Model accuracy
- Model reliability
:::
:::

::: {.column width="33%"}

##### More advanced 
::: {.nonincremental}

- Classification
- Feature engineering (preprocessing) 
- Inference for regression
- Mixed-effect models
::: 
:::

::::

## Model building overview {.smaller}

- **Model specification**: what is the form?
- **Model fitting**: you have the form, how do you guess the free parameters? 
- **Model accuracy**: you've estimated the parameters, how well does that model describe your data? 
- **Model reliability**: when you estimate the parameters, there is some uncertainty on them

# Model specification 

a brief review

## 

:::: {.columns}

::: {.column width="67%"}
#### Types of models {.smaller}
![](/include/figures/types-of-models.png)

:::

::: {.column width="33%"}

#### Specification {.smaller}

1. Response, $y$
2. Explanatory, $x_n$
3. Functional form, $y=\beta_0 + \beta_1x_1 + \epsilon$
4. Model terms
    - Intercept
    - Main
    - Interaction
    - Transformation

:::

::::


## Linear model functional form

| field | linear model eq |
| --- | --------- |
| `h.s. algebra` | $y=ax+b$ |
| `machine learning` | $y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$ |
| `statistics` | $y = β_0 + β_1x_1 + β_2x_2 + ... + β_nx_n + ε$ |
| `matrix` | $y = Xβ + ε$ |

# Model fitting

## Model fitting

```{mermaid}
%%| echo: false
flowchart TD
    spec(Model specification) --> fit(Estimate free parameters) 
    fit(Estimate free parameters) --> fitted(Fitted model) 
```

## Fitting a linear model {.smaller}

:::: {.columns}

::: {.column width="33%"}


```{mermaid}
%%| echo: false
flowchart TD
    spec(Model specification \n y = ax + b) --> fit(Estimate free parameters) 
    fit(Estimate free parameters) --> fitted(Fitted model \n y = 0.7x + 0.6) 
```


:::

::: {.column width="67%"}

```{r}
#| echo: false

data <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(1.2, 2.5, 2.3, 3.1, 4.4)
)

data2 <- tibble(
    x = c(1, 2, 3, 4, 5),
    y = c(2, 2.5, 3.3, 4.1, 6.4)


)


```

```{r}
ggplot(data, aes(x = x, y = y)) +
    geom_point(size = 4, color = "darkred") +
    geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE) 

```

:::



::::

## Fitting by intuition {.smaller}

How would you draw a "best fit" line?  


```{r}
#| echo: false

ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred")

```

## Fitting by intuition {.smaller}

Which line fits best?  How can you tell? 

```{r}
#| echo: false
#| layout-ncol: 2
#| layout-nrow: 3



ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE) +
coord_cartesian(ylim = c(0, 7)) +
labs(tag = "A")


ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(
    data = data2, 
    mapping = aes(x = x, y = y), 
    method = "lm", formula = "y ~ x", se = FALSE) +
    coord_cartesian(ylim = c(0, 7)) +
labs(tag = "B")

```


## Quantifying "goodness" of fit  {.smaller}

We can measure how close the model is to the data

```{r}
#| echo: false
modelA <- lm(y ~ x, data = data)
modelB <- lm(y ~ x, data = data2)

mseA <- data %>% add_predictions(modelA) %>%
    mutate(err = y - pred, sq_err = err^2)

mseB <- data %>% add_predictions(modelB) %>%
    mutate(err = y - pred, sq_err = err^2)

```




```{r}
#| echo: false
#| layout-ncol: 2

ggplot(mseA, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE) +
geom_segment(aes(xend = x, yend = pred)) +
coord_cartesian(ylim = c(0, 7))  +
labs(tag = "A", title = "Fits well")



ggplot(mseB, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(
    data = data2, 
    mapping = aes(x = x, y = y), 
    method = "lm", formula = "y ~ x", se = FALSE) +
    geom_segment(aes(xend = x, yend = pred)) +
    coord_cartesian(ylim = c(0, 7)) +
labs(tag = "B", title = "Fits less well")



```

. . . 

`residuals`

## $SSE=\sum_{i=i}^{n} (d_{i} - m_{i})^2$ {.smaller} 

```{r}
#| echo: false
#| layout-ncol: 2
#| layout-nrow: 2
#| 
ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(method = "lm", formula = 'y ~ x', se = FALSE) +
coord_cartesian(ylim = c(0, 7)) +
labs(tag = "A", title = "Low SSE, y = 0.7x + 0.6", caption = "SSE = 0.6")


ggplot(data, aes(x = x, y = y)) +
geom_point(size = 4, color = "darkred") +
geom_smooth(
    data = data2, 
    mapping = aes(x = x, y = y), 
    method = "lm", formula = "y ~ x", se = FALSE) +
    coord_cartesian(ylim = c(0, 7)) +
labs(tag = "B", title = "High SSE", caption = "SSE = 6.364")

kable(mseA)
kable(mseB)




```

## But there are infinite possibilities

We can't test all `Inf` of the possible free parameters

$y=b_0+b_1x_1$

:::: {.columns}

::: {.column width="50%"}

#### Free parameters to test

![](../include/figures/error-surf-1.png)

:::

::: {.column width="50%"}

#### Level = SSE

![](../include/figures/error-surf-2.png)

:::

::::


## Error surface 

![](../include/figures/error-surface.png)

## Gradient descent, intuition

![](../include/figures/grad-desc-intuition.jpeg)

## Gradient descent 

![](../include/figures/gradient-descent.png)

## Gradient descent linear model 

![](../include/figures/error-surface-linear.png)

Linear models are convex functions: one minimum

## Ordinary least squares

Linear models have a solution: we can solve for the values with linear algebra. 

:::: {.columns}

::: {.column width="50%"}

#### $y = ax + b$

$1.2 = a1 + b$

$2.5 = a2 + b$

::: 

::: {.column width="50%"}

```{r}
lm(y ~ 1 + x, data)
```

```{r}
data %>%
    specify(y ~ 1 + x) %>%
    fit()
```

:::

::::

`ordinary least squares`












