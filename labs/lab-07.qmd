---
title:  "Practice quiz 3"
subtitle: "Not graded, just practice"
date:   2023-10-26
author: Katie Schuler
number-sections: false
---

```{r}
#| echo: false
#| message: false

library(webexercises)
library(tidyverse)
library(optimg)
library(tidymodels)
theme_set(theme_gray(base_size = 16))
set.seed(60)

data <- tibble(
    y = rnorm(1000, 5, 1), 
    noise = rnorm(1000, 10, 4), 
    x = y + 10 + noise
)
```

## Model fitting 

1. True or false, gradient descent and orinary least squares are both iterative optimization algorithms.

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c("True", answer="False")

cat(longmcq(choices))

```


2. What cost function have we been using to perform our gradient descent? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "standard deviation",
    "bootstrapping",
    answer="sum of squared error",
    "absolute error"
)

cat(longmcq(choices))
```

3. True or false, when performing gradient descent on the model given by the equation $y = w_0 + w_1x_1 + w_2x_2$, we might arrive at a local minimum and miss the global one.


```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c("True", answer="False")

cat(longmcq(choices))

```

4. Which of the following would work to estimate the free parameters of a **nonlinear** model? 


```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    answer="gradient descent",
    "ordinary least squares solution",
    "both work"
)

cat(longmcq(choices))
```

5. True or false, in gradient descent, we search through all possible parameters in the parameter space. 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c("True", answer="False")

cat(longmcq(choices))

```


## Model fitting in R 

Questions 6-9 refer to the code and output below, performing gradient descent with `optimg`: 

```{r}
#| echo: false 

SSE <- function(data, par) {
  data %>%
    mutate(prediction = par[1] + par[2]* x) %>%
    mutate(error = prediction - y) %>%
    mutate(squared_error = error^2) %>%
    with(sum(squared_error))
}

```

```{r}
optimg(data = data, par = c(0,0), fn=SSE, method = "STGD")
```

6. How many steps did the gradient descent algorithm take? `r fitb(6, width =10)`

7. What was the sum of squared error of the optimal paramters?  `r fitb(959.4293, width =10)`

8. What coefficients does the algorithm converge on? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    answer = "3.37930046, 0.06683237",
    "0, 0",
    "959.4293", 
    "6, 0", 
    "all of the above"
)

cat(longmcq(choices))
```

9. What parameters were used to initialized the algorithm? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "3.37930046, 0.06683237",
    answer ="0, 0",
    "959.4293", 
    "6, 0"
)

cat(longmcq(choices))
```

Questions 10-12 refer to the output below from `lm()`: 

```{r}
#| echo: false
lm(y ~ x, data = data)

```


10. Use R notation to write the model specification. 

```{r}
#| code-fold: true
#| code-summary: "answer"
#| eval: false
y ~ x  # this works (implicit intercept)

y ~ 1 + x # this also works (explicit intercept)

```


11. Given the model is specified by the equation $y = w_0+w_1x_1$, what is the parameter estimate for $w_0$ = `r fitb(3.37822, width = 10)` and $w_1$ = `r fitb(0.06688, width = 10)`. 


12. True or false, for this model, `optimg()` with gradient descent would converge on the same parameter estimates? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(answer="True", "False")

cat(longmcq(choices))

```


## Model accuracy 

Question 13 refers to the following figure: 

```{r}
#| echo: false

set.seed(1337)
beta0 = 2
beta1 = 0.5
data <- tibble(x = runif(200, min = 0, max = 10),
               y = beta0+ (beta1*x) + rnorm(200, sd = 0.5))


```

```{r}
#| echo: false

model <- lm(y ~ 1 + x, data = data)
model_int <- lm(y ~ 1, data = data)

data <- data %>%
    mutate(mod_pred = predict(model)) %>%
    mutate(mod_int = predict(model_int))

ggplot(
    data, aes(x = x, y = y) 
) +
    geom_point() +
    geom_segment(aes(xend = x, yend = mod_pred), color = "red") +
    geom_smooth(method="lm", se = FALSE, formula = "y ~ x") 

```


13. In the figure above, which of the following corresponds to the residuals? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "blue line",
    answer ="red lines",
    "black dots", 
    "none of the above"
)

cat(longmcq(choices))

```


14. Suppose the $R^2$ value on the model in the figure above is about 0.88. Given this value, which of the following best describes the accuracy of the model? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    answer = "fairly high accuracy ",
    "fairly low accuracy",
    "not enough information"
)

cat(longmcq(choices))

```

15. Suppose 0.88 reflects the $R^2$ for our fitted model on our sample. Which of the following is true about the $R^2$ for our fitted model on the population? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "tends to be higher",
    answer = "tends to be lower",
    "the same"
)

cat(longmcq(choices))

```

16. Which of the following best describes an overfit model? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "performs well predicting new values, but poorly on the sample",
    answer = "performs well on the sample, but poorly predicting new values",
    "performs poorly both on the sample and predicting new values",
    "performs well both on the sample and predicting new values"
)

cat(longmcq(choices))

```


17. How can we estimate $R^2$ on the population? Choose all that apply.

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    answer="cross validation",
    answer = "bootstrapping",
    "regression",
    "set.seed"
)

cat(longmcq(choices))

```

18. Fill in the blanks below to best describe cross validation:
    - Leave some out
    - Fit a model on the data `r mcq(c("left out", answer =  "kept in"))`
    - Evaluate the mdoel on the data `r mcq(c(answer ="left out","kept in"))` 

## Model accuracy in R 


Questions 19-20 refer to the following code and output:

```{r}
model <- lm(y ~ 1 + x, data)
summary(model)
```

19. What is the $R^2$ value for the model fit above? 

`r fitb(0.8822, width=10)`

20. Does the value you entered in 19 reflect $R^2$ on the population or on the sample? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "population",
    answer = "sample"
)

cat(longmcq(choices))

```


Questions 21-23 refer to the following code and output: 

```{r}
# we divide the data into v folds for cross-validation
set.seed(2) 
splits <- vfold_cv(data)

# model secification 
model_spec <- 
  linear_reg() %>%  
  set_engine(engine = "lm")  

# add a workflow
our_workflow <- 
  workflow() %>%
  add_model(model_spec) %>%  
  add_formula(y ~ x) 

# fit models to our folds 
fitted_models <- 
  fit_resamples(
    object = our_workflow, 
    resamples = splits
    ) 

fitted_models %>%
    collect_metrics()

```

21. In the cross-validation performed above, how many folds were the data split into? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "2",
   "5",
    answer = "10"
)

cat(longmcq(choices))

```

22. What $R^2$ do we estimate for the population? 

`r fitb(0.890, width = 10)`

23. What model has been fit? 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    answer = "`y ~ x`",
    "`y ~ x^2`",
   "`x ~ y`",
    "`vfold_cv`"
)

cat(longmcq(choices))

```


Questions 24-26 refer to the following code and output: 

```{r}
# we bootstrap the data for cross-validation
set.seed(2) 
bootstrap <- bootstraps(data) 

# fit models to our folds 
fitted_models_boot <- 
  fit_resamples(
    object = our_workflow, 
    resamples = bootstrap
    ) 

fitted_models_boot %>%
    collect_metrics()

```

24. How many bootstrap samples did we generate? 

`r fitb(25, width = 10)`

25. True or false, we fit the same model to the bootstrap data as we did in the cross-validation code. 

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    answer = "TRUE",
    "FALSE"
)

cat(longmcq(choices))

```

26. True or false, the $R^2$ estimated by bootstrapping is equal to the $R^2$ estimated by cross-validation.

```{r}
#| echo: false
#| results: asis

# Define the answer choices
choices <- c(
    "TRUE",
    answer="FALSE"
)

cat(longmcq(choices))

```